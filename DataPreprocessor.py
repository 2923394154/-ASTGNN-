#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ASTGNNæ•°æ®é¢„å¤„ç†å™¨
æ•´åˆè‚¡ä»·æ•°æ®å’ŒBarraå› å­æ•°æ®ï¼Œä¸ºASTGNNé¡¹ç›®æä¾›æ ‡å‡†åŒ–çš„è®­ç»ƒæ•°æ®
"""

import pandas as pd
import numpy as np
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Any
import time
import warnings
import logging
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler, RobustScaler
from scipy import stats
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class ASTGNNDataPreprocessor:
    """ASTGNNæ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, config: Optional[Dict] = None, use_gpu: Optional[bool] = None):
        """åˆå§‹åŒ–é¢„å¤„ç†å™¨"""
        self.config = config or self._get_stock_friendly_config()
        
        # GPUåŠ é€Ÿé…ç½®
        if use_gpu is None:
            use_gpu = torch.cuda.is_available()
        self.use_gpu = use_gpu and torch.cuda.is_available()
        
        if self.use_gpu:
            self.device = torch.device('cuda')
            torch.cuda.empty_cache()
            logger.info(f"GPUåŠ é€Ÿå·²å¯ç”¨ - è®¾å¤‡: {torch.cuda.get_device_name()}")
        else:
            self.device = torch.device('cpu')
            logger.info("ä½¿ç”¨CPUå¤„ç†")
        
        # æ–‡ä»¶è·¯å¾„
        self.stock_file = self.config.get('stock_file', 'stock_price_vol_d.txt')
        self.barra_file = self.config.get('barra_file', 'barra_Exposure(2).')
        
        # æ•°æ®ç¼“å­˜
        self.stock_data = None
        self.barra_data = None
        self.merged_data = None
        self.processed_data = None
        
        # æ ‡å‡†åŒ–å™¨
        self.factor_scaler = StandardScaler()
        self.return_scaler = RobustScaler()
        
        # GPUæ‰¹å¤„ç†å¤§å°é…ç½®
        self.gpu_batch_size = self.config.get('gpu_batch_size', 1000)
        
        logger.info("æ•°æ®é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _get_stock_friendly_config(self) -> Dict:
        """è·å–è‚¡ç¥¨å‹å¥½çš„é…ç½®ï¼Œç§»é™¤æ‰€æœ‰ç¡¬ç¼–ç è‚¡ç¥¨æ•°é‡é™åˆ¶"""
        return {
            'sequence_length': 20,          # åºåˆ—é•¿åº¦
            'prediction_horizon': 10,       # é¢„æµ‹æ—¶é—´è·¨åº¦
            'batch_size': 1000,            # æ‰¹æ¬¡å¤§å°
            'overlap_ratio': 0.5,          # åºåˆ—é‡å æ¯”ä¾‹
            'min_stock_history': 30,       # ğŸ”§ é™ä½æœ€å°å†å²è¦æ±‚ï¼šä»60å¤©é™åˆ°30å¤©
            'max_missing_ratio': 0.3,      # æœ€å¤§ç¼ºå¤±ç‡
            'outlier_threshold': 5.0,      # å¼‚å¸¸å€¼é˜ˆå€¼
            'remove_outliers': True,       # ğŸ”§ æ·»åŠ ç¼ºå¤±çš„é…ç½®ï¼šå¯ç”¨å¼‚å¸¸å€¼ç§»é™¤
            'adjacency_threshold': 0.10,   # ğŸ”§ æ·»åŠ ç¼ºå¤±çš„é…ç½®ï¼šé‚»æ¥çŸ©é˜µé˜ˆå€¼
            'factor_standardization': True,    # ğŸ”§ æ·»åŠ ç¼ºå¤±çš„é…ç½®ï¼šå› å­æ ‡å‡†åŒ–
            'return_standardization': True,    # ğŸ”§ æ·»åŠ ç¼ºå¤±çš„é…ç½®ï¼šæ”¶ç›Šç‡æ ‡å‡†åŒ–
            'standardize_features': True,   # ç‰¹å¾æ ‡å‡†åŒ–
            'data_split_ratio': [0.7, 0.15, 0.15],  # è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ¯”ä¾‹
            'cpu_memory_limit_gb': 24.0,   # ğŸ”§ å¤§å¹…æå‡CPUå†…å­˜é™åˆ¶ï¼šä»6GBæå‡åˆ°24GB
            'gpu_memory_threshold': 0.8,   # GPUå†…å­˜ä½¿ç”¨é˜ˆå€¼
            'adjacency_memory_threshold': 0.6,  # é‚»æ¥çŸ©é˜µå†…å­˜é˜ˆå€¼
            'enable_gpu_acceleration': True,     # å¯ç”¨GPUåŠ é€Ÿ
            'memory_safety_factor': 1.5,        # å†…å­˜å®‰å…¨ç³»æ•°
            'min_stocks_for_gpu': 1000,         # GPUå¤„ç†æœ€å°è‚¡ç¥¨æ•°
            'enable_mixed_precision': False,    # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜ï¼‰
            'enable_memory_mapping': True,      # å†…å­˜æ˜ å°„
            'chunk_size': 10000,                # æ•°æ®å—å¤§å°
            'max_workers': 8,                   # æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
            'progress_update_interval': 100,    # è¿›åº¦æ›´æ–°é—´éš”
            'enable_data_validation': True,     # å¯ç”¨æ•°æ®éªŒè¯
            'remove_duplicates': True,          # ç§»é™¤é‡å¤æ•°æ®
            'handle_missing_values': 'interpolate',  # ç¼ºå¤±å€¼å¤„ç†æ–¹å¼
            'winsorize_quantile': 0.01,         # æå€¼å¤„ç†åˆ†ä½æ•°
            'correlation_threshold': 0.95,      # ç›¸å…³æ€§é˜ˆå€¼
            'volume_filter_percentile': 10      # æˆäº¤é‡è¿‡æ»¤ç™¾åˆ†ä½æ•°
        }
    
    def load_stock_data(self, sample_size: Optional[int] = None, 
                        target_stocks: Optional[List[str]] = None,
                        target_date_range: Optional[Tuple[str, str]] = None) -> Optional[pd.DataFrame]:
        """åŠ è½½è‚¡ä»·æ•°æ®"""
        logger.info("å¼€å§‹åŠ è½½è‚¡ä»·æ•°æ®")
        
        try:
            # ä½¿ç”¨featheræ ¼å¼è¯»å–
            df = pd.read_feather(self.stock_file)
            logger.info(f"æˆåŠŸåŠ è½½è‚¡ä»·æ•°æ®: {df.shape}")
            
            # æ•°æ®æ¸…ç†
            df['date'] = pd.to_datetime(df['date'])
            
            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡è‚¡ç¥¨ï¼Œä¼˜å…ˆè¿‡æ»¤
            if target_stocks:
                df = df[df['StockID'].isin(target_stocks)]
                logger.info(f"æŒ‰ç›®æ ‡è‚¡ç¥¨è¿‡æ»¤å: {df.shape}")
            
            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡æ—¥æœŸèŒƒå›´ï¼Œè¿‡æ»¤æ—¥æœŸ
            if target_date_range:
                start_date, end_date = target_date_range
                df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]
                logger.info(f"æŒ‰ç›®æ ‡æ—¥æœŸè¿‡æ»¤å: {df.shape}")
            
            # ç§»é™¤é‡‡æ ·é™åˆ¶ï¼Œä¿ç•™æ‰€æœ‰æ•°æ®
            if sample_size and len(df) > sample_size:
                logger.info(f"æ•°æ®é‡è¾ƒå¤§({len(df):,}è¡Œ)ï¼Œä½†å·²ç¦ç”¨é‡‡æ ·ä»¥ä¿ç•™æ‰€æœ‰è‚¡ç¥¨")
                logger.info(f"ä¿ç•™æ‰€æœ‰ {df['StockID'].nunique()} åªè‚¡ç¥¨çš„å®Œæ•´æ•°æ®")
            
            # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            date_range = (df['date'].min(), df['date'].max())
            unique_stocks = df['StockID'].nunique()
            missing_values = df.isnull().sum().sum()
            
            logger.info(f"æ—¶é—´èŒƒå›´: {date_range[0]} åˆ° {date_range[1]}")
            logger.info(f"è‚¡ç¥¨æ•°é‡: {unique_stocks:,}")
            logger.info(f"ç¼ºå¤±å€¼æ€»æ•°: {missing_values}")
            
            self.stock_data = df
            return df
            
        except Exception as e:
            logger.error(f"åŠ è½½è‚¡ä»·æ•°æ®å¤±è´¥: {str(e)}")
            return None
    
    def load_barra_data(self, sample_size: Optional[int] = None,
                        target_stocks: Optional[List[str]] = None,
                        target_date_range: Optional[Tuple[str, str]] = None) -> Optional[pd.DataFrame]:
        """åŠ è½½Barraå› å­æ•°æ®"""
        logger.info("å¼€å§‹åŠ è½½Barraå› å­æ•°æ®")
        
        try:
            # ä½¿ç”¨parquetæ ¼å¼è¯»å–
            df = pd.read_parquet(self.barra_file)
            logger.info(f"æˆåŠŸåŠ è½½Barraæ•°æ®: {df.shape}")
            
            # æ•°æ®æ¸…ç†å’Œé‡å‘½å
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
            df = df.rename(columns={
                'æ—¥æœŸ': 'date',
                'è‚¡ç¥¨ä»£ç ': 'StockID'
            })
            
            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡è‚¡ç¥¨ï¼Œä¼˜å…ˆè¿‡æ»¤
            if target_stocks:
                df = df[df['StockID'].isin(target_stocks)]
                logger.info(f"æŒ‰ç›®æ ‡è‚¡ç¥¨è¿‡æ»¤å: {df.shape}")
            
            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡æ—¥æœŸèŒƒå›´ï¼Œè¿‡æ»¤æ—¥æœŸ
            if target_date_range:
                start_date, end_date = target_date_range
                df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]
                logger.info(f"æŒ‰ç›®æ ‡æ—¥æœŸè¿‡æ»¤å: {df.shape}")
            
            # ç§»é™¤é‡‡æ ·é™åˆ¶ï¼Œä¿ç•™æ‰€æœ‰Barraæ•°æ®
            if sample_size and len(df) > sample_size:
                logger.info(f"Barraæ•°æ®é‡è¾ƒå¤§({len(df):,}è¡Œ)ï¼Œä½†å·²ç¦ç”¨é‡‡æ ·ä»¥ä¿ç•™æ‰€æœ‰è‚¡ç¥¨")
                logger.info(f"ä¿ç•™æ‰€æœ‰ {df['StockID'].nunique()} åªè‚¡ç¥¨çš„å®Œæ•´Barraæ•°æ®")
            
            # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            date_range = (df['date'].min(), df['date'].max())
            unique_stocks = df['StockID'].nunique()
            factor_count = len([col for col in df.columns if col.startswith('Exposure_')])
            
            logger.info(f"æ—¶é—´èŒƒå›´: {date_range[0]} åˆ° {date_range[1]}")
            logger.info(f"è‚¡ç¥¨æ•°é‡: {unique_stocks:,}")
            logger.info(f"å› å­æ•°é‡: {factor_count}")
            
            self.barra_data = df
            return df
            
        except Exception as e:
            logger.error(f"åŠ è½½Barraæ•°æ®å¤±è´¥: {str(e)}")
            return None
    
    def merge_datasets(self, date_range: Optional[Tuple[str, str]] = None) -> Optional[pd.DataFrame]:
        """åˆå¹¶è‚¡ä»·æ•°æ®å’ŒBarraå› å­æ•°æ®"""
        logger.info("å¼€å§‹åˆå¹¶æ•°æ®é›†")
        
        if self.stock_data is None or self.barra_data is None:
            logger.error("è¯·å…ˆåŠ è½½è‚¡ä»·æ•°æ®å’ŒBarraæ•°æ®")
            return None
        
        # æ—¶é—´èŒƒå›´è¿‡æ»¤
        if date_range:
            start_date, end_date = date_range
            stock_filtered = self.stock_data[
                (self.stock_data['date'] >= start_date) & 
                (self.stock_data['date'] <= end_date)
            ].copy()
            barra_filtered = self.barra_data[
                (self.barra_data['date'] >= start_date) & 
                (self.barra_data['date'] <= end_date)
            ].copy()
            logger.info(f"åº”ç”¨æ—¶é—´è¿‡æ»¤: {start_date} åˆ° {end_date}")
        else:
            stock_filtered = self.stock_data.copy()
            barra_filtered = self.barra_data.copy()
        
        # åˆå¹¶æ•°æ®
        merged = pd.merge(
            stock_filtered, 
            barra_filtered,
            on=['date', 'StockID'],
            how='inner'
        )
        
        logger.info(f"åˆå¹¶å®Œæˆ: {merged.shape}")
        if len(merged) > 0:
            logger.info(f"åˆå¹¶åæ—¶é—´èŒƒå›´: {merged['date'].min()} åˆ° {merged['date'].max()}")
            logger.info(f"åˆå¹¶åè‚¡ç¥¨æ•°é‡: {merged['StockID'].nunique():,}")
        
        self.merged_data = merged
        return merged
    
    def calculate_returns_and_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ”¶ç›Šç‡å’ŒæŠ€æœ¯æŒ‡æ ‡"""
        logger.info("è®¡ç®—æ”¶ç›Šç‡å’ŒæŠ€æœ¯æŒ‡æ ‡")
        
        # é‡ç½®ç´¢å¼•å¹¶æŒ‰è‚¡ç¥¨åˆ†ç»„æ’åº
        data = data.reset_index(drop=True)
        data = data.sort_values(['StockID', 'date'])
        
        # è®¡ç®—å¤šæœŸæ”¶ç›Šç‡
        return_periods = [1, 5, 10, 20]
        for period in return_periods:
            col_name = f'return_{period}d'
            data[col_name] = data.groupby('StockID')['close'].pct_change(period)
        
        # è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡
        log_returns = data.groupby('StockID')['close'].transform(lambda x: np.log(x / x.shift(1)))
        data['log_return'] = log_returns
        
        # è®¡ç®—æ³¢åŠ¨ç‡æŒ‡æ ‡
        data['volatility_5d'] = data.groupby('StockID')['log_return'].transform(lambda x: x.rolling(5).std())
        data['volatility_20d'] = data.groupby('StockID')['log_return'].transform(lambda x: x.rolling(20).std())
        
        # è®¡ç®—ä»·æ ¼æŠ€æœ¯æŒ‡æ ‡
        data['price_ma_5'] = data.groupby('StockID')['close'].transform(lambda x: x.rolling(5).mean())
        data['price_ma_20'] = data.groupby('StockID')['close'].transform(lambda x: x.rolling(20).mean())
        data['price_ratio_ma5'] = data['close'] / data['price_ma_5']
        data['price_ratio_ma20'] = data['close'] / data['price_ma_20']
        
        # è®¡ç®—æˆäº¤é‡æŒ‡æ ‡
        data['volume_ma_5'] = data.groupby('StockID')['vol'].transform(lambda x: x.rolling(5).mean())
        data['volume_ratio'] = data['vol'] / data['volume_ma_5']
        
        # è®¡ç®—momentumæŒ‡æ ‡
        data['momentum_5d'] = data.groupby('StockID')['close'].transform(lambda x: x / x.shift(5) - 1)
        data['momentum_20d'] = data.groupby('StockID')['close'].transform(lambda x: x / x.shift(20) - 1)
        
        logger.info("æ”¶ç›Šç‡å’ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆ")
        return data
    
    def remove_outliers_stock_friendly(self, data: pd.DataFrame) -> pd.DataFrame:
        """å¿«é€Ÿçš„è‚¡ç¥¨å‹å¥½å¼‚å¸¸å€¼å¤„ç†æ–¹æ³• - å‘é‡åŒ–ä¼˜åŒ–ç‰ˆæœ¬"""
        if not self.config['remove_outliers']:
            return data
        
        logger.info("å¼€å§‹å¿«é€Ÿè‚¡ç¥¨å‹å¥½å¼‚å¸¸å€¼å¤„ç†ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ï¼‰")
        initial_count = len(data)
        
        # è·å–æ•°å€¼åˆ—ï¼ˆæ’é™¤æ ‡è¯†åˆ—ï¼‰
        numeric_cols = [col for col in data.select_dtypes(include=[np.number]).columns 
                       if col not in ['date', 'StockID']]
        
        threshold = self.config['outlier_threshold']
        outlier_method = self.config.get('outlier_method', 'stock_wise')
        use_winsorize = self.config.get('outlier_winsorize', True)
        winsorize_limits = self.config.get('winsorize_limits', (0.01, 0.01))
        
        if outlier_method == 'stock_wise':
            logger.info(f"ä½¿ç”¨å‘é‡åŒ–æŒ‰è‚¡ç¥¨åˆ†ç»„å¤„ç†ï¼Œå¤„ç† {len(numeric_cols)} ä¸ªæ•°å€¼åˆ—")
            
            if use_winsorize:
                # å‘é‡åŒ–ç¼©å°¾å¤„ç†
                from scipy.stats import mstats
                
                def winsorize_group(group):
                    """å¯¹å•ä¸ªè‚¡ç¥¨ç»„è¿›è¡Œç¼©å°¾å¤„ç†"""
                    result = group.copy()
                    for col in numeric_cols:
                        if col in result.columns and result[col].notna().sum() > 10:
                            # å¡«å……ç¼ºå¤±å€¼åç¼©å°¾å¤„ç†
                            filled_values = result[col].fillna(result[col].median())
                            result[col] = mstats.winsorize(filled_values, limits=winsorize_limits)
                    return result
                
                # ä½¿ç”¨groupbyå‘é‡åŒ–å¤„ç†
                data = data.groupby('StockID', group_keys=False).apply(winsorize_group)
                
            else:
                # å‘é‡åŒ–Z-scoreå¤„ç†
                def filter_outliers_group(group):
                    """å¯¹å•ä¸ªè‚¡ç¥¨ç»„è¿›è¡Œå¼‚å¸¸å€¼è¿‡æ»¤"""
                    result = group.copy()
                    
                    # è®¡ç®—æ‰€æœ‰æ•°å€¼åˆ—çš„Z-scoreï¼ˆå‘é‡åŒ–ï¼‰
                    for col in numeric_cols:
                        if col in result.columns and result[col].notna().sum() > 10:
                            col_data = result[col]
                            z_scores = np.abs((col_data - col_data.mean()) / (col_data.std() + 1e-8))
                            # åªä¿ç•™æ‰€æœ‰åˆ—éƒ½ä¸æ˜¯å¼‚å¸¸å€¼çš„è¡Œ
                            if 'outlier_mask' not in locals():
                                outlier_mask = z_scores <= threshold
                            else:
                                outlier_mask = outlier_mask & (z_scores <= threshold)
                    
                    if 'outlier_mask' in locals():
                        result = result[outlier_mask]
                    
                    return result
                
                # ä½¿ç”¨groupbyå‘é‡åŒ–å¤„ç†
                data = data.groupby('StockID', group_keys=False).apply(filter_outliers_group)
                
        else:
            # å…¨å±€å‘é‡åŒ–å¼‚å¸¸å€¼å¤„ç†
            logger.info("ä½¿ç”¨å…¨å±€å‘é‡åŒ–å¼‚å¸¸å€¼å¤„ç†")
            
            if use_winsorize:
                from scipy.stats import mstats
                for col in numeric_cols:
                    filled_values = data[col].fillna(data[col].median())
                    data[col] = mstats.winsorize(filled_values, limits=winsorize_limits)
            else:
                # å‘é‡åŒ–å…¨å±€Z-scoreè®¡ç®—
                outlier_mask = pd.Series(True, index=data.index)
                for col in numeric_cols:
                    z_scores = np.abs((data[col] - data[col].mean()) / (data[col].std() + 1e-8))
                    outlier_mask = outlier_mask & (z_scores <= threshold)
                
                data = data[outlier_mask]
        
        final_count = len(data)
        removed_count = initial_count - final_count
        
        logger.info(f"å¿«é€Ÿå¼‚å¸¸å€¼å¤„ç†å®Œæˆ: ç§»é™¤ {removed_count} è¡Œ ({removed_count/initial_count*100:.2f}%)")
        logger.info(f"ä¿ç•™è‚¡ç¥¨æ•°é‡: {data['StockID'].nunique()}")
        return data

    def gpu_accelerated_remove_outliers(self, data: pd.DataFrame) -> pd.DataFrame:
        """GPUåŠ é€Ÿçš„å¼‚å¸¸å€¼å¤„ç†æ–¹æ³•"""
        if not self.config['remove_outliers']:
            return data
        
        if not self.use_gpu or len(data) < 10000:
            logger.info("æ•°æ®é‡è¾ƒå°æˆ–GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUç‰ˆæœ¬å¼‚å¸¸å€¼å¤„ç†")
            return self.remove_outliers_stock_friendly(data)
        
        logger.info("å¼€å§‹GPUåŠ é€Ÿå¼‚å¸¸å€¼å¤„ç†")
        initial_count = len(data)
        
        # è·å–æ•°å€¼åˆ—
        numeric_cols = [col for col in data.select_dtypes(include=[np.number]).columns 
                       if col not in ['date', 'StockID']]
        
        threshold = self.config['outlier_threshold']
        use_winsorize = self.config.get('outlier_winsorize', True)
        winsorize_limits = self.config.get('winsorize_limits', (0.01, 0.01))
        
        try:
            # åˆ›å»ºè‚¡ç¥¨IDæ˜ å°„
            unique_stocks = data['StockID'].unique()
            stock_to_idx = {stock: idx for idx, stock in enumerate(unique_stocks)}
            data['stock_idx'] = data['StockID'].map(stock_to_idx)
            
            # æ‰¹é‡GPUå¤„ç†
            batch_size = min(1000, len(unique_stocks))
            processed_data_list = []
            
            for batch_start in range(0, len(unique_stocks), batch_size):
                batch_end = min(batch_start + batch_size, len(unique_stocks))
                batch_stocks = unique_stocks[batch_start:batch_end]
                batch_data = data[data['StockID'].isin(batch_stocks)].copy()
                
                if use_winsorize:
                    # GPUåŠ é€Ÿç¼©å°¾å¤„ç†
                    batch_processed = self._gpu_winsorize_batch(batch_data, numeric_cols, winsorize_limits)
                else:
                    # GPUåŠ é€ŸZ-scoreè¿‡æ»¤
                    batch_processed = self._gpu_zscore_filter_batch(batch_data, numeric_cols, threshold)
                
                processed_data_list.append(batch_processed)
                
                if batch_start // batch_size % 5 == 0:
                    logger.info(f"GPUå¼‚å¸¸å€¼å¤„ç†è¿›åº¦: {batch_end}/{len(unique_stocks)} åªè‚¡ç¥¨")
            
            # åˆå¹¶å¤„ç†ç»“æœ
            data = pd.concat(processed_data_list, ignore_index=True)
            data = data.drop(columns=['stock_idx'])
            
            # GPUå†…å­˜æ¸…ç†
            torch.cuda.empty_cache()
            
        except Exception as e:
            logger.warning(f"GPUå¼‚å¸¸å€¼å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°CPU: {str(e)}")
            data = self.remove_outliers_stock_friendly(data)
        
        final_count = len(data)
        removed_count = initial_count - final_count
        
        logger.info(f"GPUå¼‚å¸¸å€¼å¤„ç†å®Œæˆ: ç§»é™¤ {removed_count} è¡Œ ({removed_count/initial_count*100:.2f}%)")
        return data
    
    def _gpu_winsorize_batch(self, batch_data: pd.DataFrame, numeric_cols: list, winsorize_limits: tuple) -> pd.DataFrame:
        """GPUæ‰¹é‡ç¼©å°¾å¤„ç†"""
        for col in numeric_cols:
            if col in batch_data.columns and batch_data[col].notna().sum() > 10:
                try:
                    # è½¬ç§»åˆ°GPU
                    values = batch_data[col].fillna(batch_data[col].median()).values
                    values_tensor = torch.tensor(values, dtype=torch.float32, device=self.device)
                    
                    # GPUè®¡ç®—åˆ†ä½æ•°
                    lower_quantile = torch.quantile(values_tensor, winsorize_limits[0])
                    upper_quantile = torch.quantile(values_tensor, 1 - winsorize_limits[1])
                    
                    # GPUç¼©å°¾å¤„ç†
                    values_tensor = torch.clamp(values_tensor, lower_quantile, upper_quantile)
                    
                    # å›ä¼ åˆ°CPU
                    batch_data[col] = values_tensor.cpu().numpy()
                    
                except Exception:
                    # GPUå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°scipy
                    from scipy.stats import mstats
                    filled_values = batch_data[col].fillna(batch_data[col].median())
                    batch_data[col] = mstats.winsorize(filled_values, limits=winsorize_limits)
        
        return batch_data
    
    def _gpu_zscore_filter_batch(self, batch_data: pd.DataFrame, numeric_cols: list, threshold: float) -> pd.DataFrame:
        """GPUæ‰¹é‡Z-scoreè¿‡æ»¤"""
        try:
            # å°†æ•°å€¼æ•°æ®è½¬ç§»åˆ°GPU
            numeric_data = batch_data[numeric_cols].fillna(0).values
            data_tensor = torch.tensor(numeric_data, dtype=torch.float32, device=self.device)
            
            # GPUè®¡ç®—Z-scores
            means = torch.mean(data_tensor, dim=0, keepdim=True)
            stds = torch.std(data_tensor, dim=0, keepdim=True) + 1e-8
            z_scores = torch.abs((data_tensor - means) / stds)
            
            # è®¡ç®—è¿‡æ»¤æ©ç 
            outlier_mask = (z_scores <= threshold).all(dim=1)
            outlier_indices = outlier_mask.cpu().numpy()
            
            # åº”ç”¨è¿‡æ»¤
            filtered_data = batch_data[outlier_indices]
            
        except Exception:
            # GPUå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°CPUç‰ˆæœ¬
            filtered_data = batch_data.copy()
            for col in numeric_cols:
                if col in filtered_data.columns:
                    col_data = filtered_data[col].fillna(0)
                    z_scores = np.abs((col_data - col_data.mean()) / (col_data.std() + 1e-8))
                    filtered_data = filtered_data[z_scores <= threshold]
        
        return filtered_data

    def remove_outliers(self, data: pd.DataFrame) -> pd.DataFrame:
        """ç§»é™¤å¼‚å¸¸å€¼ - æ™ºèƒ½é€‰æ‹©GPU/CPUç‰ˆæœ¬"""
        if self.use_gpu and len(data) >= 10000:
            return self.gpu_accelerated_remove_outliers(data)
        else:
            return self.remove_outliers_stock_friendly(data)
    
    def gpu_accelerated_standardize_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """GPUåŠ é€Ÿç‰¹å¾æ ‡å‡†åŒ–"""
        logger.info("å¼€å§‹GPUåŠ é€Ÿç‰¹å¾æ ‡å‡†åŒ–")
        
        # è·å–å› å­åˆ—å’Œæ”¶ç›Šç‡åˆ—
        factor_cols = [col for col in data.columns if col.startswith('Exposure_')]
        return_cols = [col for col in data.columns if 'return' in col and col != 'turnoverrate']
        
        try:
            # GPUæ ‡å‡†åŒ–å› å­
            if self.config['factor_standardization'] and factor_cols:
                if self.use_gpu and len(data) >= 5000:
                    data[factor_cols] = self._gpu_standardize_columns(data, factor_cols)
                    logger.info(f"GPUå› å­æ ‡å‡†åŒ–å®Œæˆ: {len(factor_cols)} ä¸ªå› å­")
                else:
                    data[factor_cols] = self.factor_scaler.fit_transform(data[factor_cols].fillna(0))
                    logger.info(f"CPUå› å­æ ‡å‡†åŒ–å®Œæˆ: {len(factor_cols)} ä¸ªå› å­")
            
            # GPUæ ‡å‡†åŒ–æ”¶ç›Šç‡
            if self.config['return_standardization'] and return_cols:
                if self.use_gpu and len(data) >= 5000:
                    data[return_cols] = self._gpu_standardize_columns(data, return_cols)
                    logger.info(f"GPUæ”¶ç›Šç‡æ ‡å‡†åŒ–å®Œæˆ: {len(return_cols)} ä¸ªæ”¶ç›Šç‡æŒ‡æ ‡")
                else:
                    data[return_cols] = self.return_scaler.fit_transform(data[return_cols].fillna(0))
                    logger.info(f"CPUæ”¶ç›Šç‡æ ‡å‡†åŒ–å®Œæˆ: {len(return_cols)} ä¸ªæ”¶ç›Šç‡æŒ‡æ ‡")
        
        except Exception as e:
            logger.warning(f"GPUæ ‡å‡†åŒ–å¤±è´¥ï¼Œå›é€€åˆ°CPU: {str(e)}")
            return self.standardize_features(data)
        
        return data
    
    def _gpu_standardize_columns(self, data: pd.DataFrame, columns: list) -> pd.DataFrame:
        """GPUæ‰¹é‡æ ‡å‡†åŒ–åˆ—"""
        try:
            # å‡†å¤‡æ•°æ®
            col_data = data[columns].fillna(0).values
            data_tensor = torch.tensor(col_data, dtype=torch.float32, device=self.device)
            
            # GPUè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
            means = torch.mean(data_tensor, dim=0, keepdim=True)
            stds = torch.std(data_tensor, dim=0, keepdim=True) + 1e-8
            
            # GPUæ ‡å‡†åŒ–
            standardized_tensor = (data_tensor - means) / stds
            
            # å›ä¼ åˆ°CPUå¹¶åˆ›å»ºDataFrame
            standardized_data = standardized_tensor.cpu().numpy()
            result_df = pd.DataFrame(standardized_data, columns=columns, index=data.index)
            
            # GPUå†…å­˜æ¸…ç†
            torch.cuda.empty_cache()
            
            return result_df
            
        except Exception as e:
            logger.warning(f"GPUæ ‡å‡†åŒ–åˆ—å¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°sklearn")
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            return pd.DataFrame(
                scaler.fit_transform(data[columns].fillna(0)),
                columns=columns,
                index=data.index
            )

    def standardize_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾æ ‡å‡†åŒ– - æ™ºèƒ½é€‰æ‹©GPU/CPUç‰ˆæœ¬"""
        if self.use_gpu and len(data) >= 5000:
            return self.gpu_accelerated_standardize_features(data)
        
        logger.info("å¼€å§‹CPUç‰¹å¾æ ‡å‡†åŒ–")
        
        # è·å–å› å­åˆ—å’Œæ”¶ç›Šç‡åˆ—
        factor_cols = [col for col in data.columns if col.startswith('Exposure_')]
        return_cols = [col for col in data.columns if 'return' in col and col != 'turnoverrate']
        
        # æ ‡å‡†åŒ–å› å­
        if self.config['factor_standardization'] and factor_cols:
            data[factor_cols] = self.factor_scaler.fit_transform(data[factor_cols].fillna(0))
            logger.info(f"å› å­æ ‡å‡†åŒ–å®Œæˆ: {len(factor_cols)} ä¸ªå› å­")
        
        # æ ‡å‡†åŒ–æ”¶ç›Šç‡
        if self.config['return_standardization'] and return_cols:
            data[return_cols] = self.return_scaler.fit_transform(data[return_cols].fillna(0))
            logger.info(f"æ”¶ç›Šç‡æ ‡å‡†åŒ–å®Œæˆ: {len(return_cols)} ä¸ªæ”¶ç›Šç‡æŒ‡æ ‡")
        
        return data
    
    def filter_stocks_by_history_optimized(self, data: pd.DataFrame) -> pd.DataFrame:
        """ä¼˜åŒ–çš„è‚¡ç¥¨å†å²æ•°æ®è¿‡æ»¤ - æ›´çµæ´»çš„ä¿ç•™ç­–ç•¥"""
        min_history = self.config['min_stock_history']
        max_missing_ratio = self.config.get('max_missing_ratio', 0.3)
        min_trading_days = self.config.get('min_trading_days', 50)
        
        logger.info(f"å¼€å§‹ä¼˜åŒ–è‚¡ç¥¨è¿‡æ»¤: æœ€å°‘{min_history}å¤©æ•°æ®ï¼Œæœ€å¤§ç¼ºå¤±ç‡{max_missing_ratio*100}%ï¼Œæœ€å°‘äº¤æ˜“{min_trading_days}å¤©")
        
        # è®¡ç®—æ¯åªè‚¡ç¥¨çš„ç»Ÿè®¡ä¿¡æ¯
        stock_stats = []
        all_dates = sorted(data['date'].unique())
        total_trading_days = len(all_dates)
        
        for stock_id in data['StockID'].unique():
            stock_data = data[data['StockID'] == stock_id]
            
            # åŸºæœ¬ç»Ÿè®¡
            actual_days = len(stock_data)
            trading_days = stock_data['date'].nunique()
            missing_ratio = 1 - (actual_days / total_trading_days)
            
            # è®¡ç®—æ•°æ®è´¨é‡æŒ‡æ ‡
            numeric_cols = [col for col in stock_data.select_dtypes(include=[np.number]).columns 
                           if col not in ['date', 'StockID']]
            
            data_quality_scores = []
            for col in numeric_cols:
                non_null_ratio = stock_data[col].notna().sum() / len(stock_data)
                data_quality_scores.append(non_null_ratio)
            
            avg_data_quality = np.mean(data_quality_scores) if data_quality_scores else 0
            
            stock_stats.append({
                'StockID': stock_id,
                'actual_days': actual_days,
                'trading_days': trading_days,
                'missing_ratio': missing_ratio,
                'data_quality': avg_data_quality,
                'meets_min_history': actual_days >= min_history,
                'meets_missing_threshold': missing_ratio <= max_missing_ratio,
                'meets_trading_days': trading_days >= min_trading_days
            })
        
        # è½¬æ¢ä¸ºDataFrameä¾¿äºåˆ†æ
        stats_df = pd.DataFrame(stock_stats)
        
        # å¤šå±‚æ¬¡è¿‡æ»¤ç­–ç•¥
        # ç­–ç•¥1: ä¸¥æ ¼æ ‡å‡†ï¼ˆåŒæ—¶æ»¡è¶³æ‰€æœ‰æ¡ä»¶ï¼‰
        strict_stocks = stats_df[
            stats_df['meets_min_history'] & 
            stats_df['meets_missing_threshold'] & 
            stats_df['meets_trading_days']
        ]['StockID'].tolist()
        
        # ç­–ç•¥2: å®½æ¾æ ‡å‡†ï¼ˆæ»¡è¶³å¤§éƒ¨åˆ†æ¡ä»¶ï¼‰
        if len(strict_stocks) == 0:  # å®Œå…¨ç§»é™¤æœ€å°‘è‚¡ç¥¨æ•°é‡é™åˆ¶
            logger.info(f"ä¸¥æ ¼æ ‡å‡†ä»…ä¿ç•™{len(strict_stocks)}åªè‚¡ç¥¨ï¼Œå¯ç”¨å®½æ¾æ ‡å‡†")
            
            # é™ä½è¦æ±‚
            relaxed_min_history = max(min_history // 2, min_trading_days)
            relaxed_missing_ratio = min(max_missing_ratio * 1.5, 0.5)
            
            relaxed_stocks = stats_df[
                (stats_df['actual_days'] >= relaxed_min_history) &
                (stats_df['missing_ratio'] <= relaxed_missing_ratio) &
                (stats_df['data_quality'] >= 0.3)  # è‡³å°‘30%çš„æ•°æ®è´¨é‡
            ]['StockID'].tolist()
            
            valid_stocks = relaxed_stocks
            logger.info(f"å®½æ¾æ ‡å‡†ä¿ç•™{len(valid_stocks)}åªè‚¡ç¥¨")
        else:
            valid_stocks = strict_stocks
            logger.info(f"ä¸¥æ ¼æ ‡å‡†ä¿ç•™{len(valid_stocks)}åªè‚¡ç¥¨")
        
        # è¿‡æ»¤æ•°æ®
        filtered_data = data[data['StockID'].isin(valid_stocks)]
        
        # æ‰“å°è¯¦ç»†ç»Ÿè®¡
        logger.info(f"è‚¡ç¥¨è¿‡æ»¤ç»Ÿè®¡:")
        logger.info(f"  åŸå§‹è‚¡ç¥¨æ•°: {len(stats_df)}")
        logger.info(f"  ä¿ç•™è‚¡ç¥¨æ•°: {len(valid_stocks)}")
        logger.info(f"  ä¿ç•™ç‡: {len(valid_stocks)/len(stats_df)*100:.1f}%")
        logger.info(f"  å¹³å‡æ•°æ®å¤©æ•°: {stats_df[stats_df['StockID'].isin(valid_stocks)]['actual_days'].mean():.0f}")
        logger.info(f"  å¹³å‡æ•°æ®è´¨é‡: {stats_df[stats_df['StockID'].isin(valid_stocks)]['data_quality'].mean():.2f}")
        
        return filtered_data

    def gpu_accelerated_filter_stocks_by_history(self, data: pd.DataFrame) -> pd.DataFrame:
        """GPUåŠ é€Ÿçš„è‚¡ç¥¨å†å²æ•°æ®è¿‡æ»¤"""
        if not self.use_gpu or len(data) < 5000:
            logger.info("æ•°æ®é‡è¾ƒå°æˆ–GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUç‰ˆæœ¬è‚¡ç¥¨è¿‡æ»¤")
            return self.filter_stocks_by_history_optimized(data)
        
        logger.info("GPUåŠ é€Ÿè‚¡ç¥¨è¿‡æ»¤å¼€å§‹")
        min_history = self.config['min_stock_history']
        max_missing_ratio = self.config.get('max_missing_ratio', 0.3)
        min_trading_days = self.config.get('min_trading_days', 50)
        
        # è·å–åŸºæœ¬ä¿¡æ¯
        all_dates = sorted(data['date'].unique())
        all_stocks = sorted(data['StockID'].unique())
        total_trading_days = len(all_dates)
        
        logger.info(f"GPUå¤„ç† {len(all_stocks)} åªè‚¡ç¥¨çš„è¿‡æ»¤åˆ†æ")
        
        # åˆ›å»ºè‚¡ç¥¨IDåˆ°ç´¢å¼•çš„æ˜ å°„
        stock_to_idx = {stock: idx for idx, stock in enumerate(all_stocks)}
        data['stock_idx'] = data['StockID'].map(stock_to_idx)
        
        # å‡†å¤‡GPUè®¡ç®—çš„æ•°æ®
        numeric_cols = [col for col in data.select_dtypes(include=[np.number]).columns 
                       if col not in ['date', 'StockID', 'stock_idx']]
        
        # è½¬ç§»åˆ°GPUè¿›è¡Œå¹¶è¡Œè®¡ç®—
        device = self.device
        
        # æ‰¹é‡å¤„ç†è‚¡ç¥¨ç»Ÿè®¡
        batch_size = min(500, len(all_stocks))
        stock_stats = []
        
        for batch_start in range(0, len(all_stocks), batch_size):
            batch_end = min(batch_start + batch_size, len(all_stocks))
            batch_stocks = all_stocks[batch_start:batch_end]
            
            # ä¸ºå½“å‰æ‰¹æ¬¡å‡†å¤‡æ•°æ®
            batch_data = data[data['StockID'].isin(batch_stocks)]
            
            # GPUå¹¶è¡Œè®¡ç®—æ¯åªè‚¡ç¥¨çš„ç»Ÿè®¡ä¿¡æ¯
            batch_stats = self._gpu_compute_stock_statistics(
                batch_data, batch_stocks, numeric_cols, total_trading_days
            )
            
            stock_stats.extend(batch_stats)
            
            if (batch_start // batch_size + 1) % 5 == 0:
                logger.info(f"GPUå¤„ç†è¿›åº¦: {batch_start + len(batch_stocks)}/{len(all_stocks)} åªè‚¡ç¥¨")
        
        # è½¬æ¢ä¸ºDataFrameè¿›è¡Œåç»­åˆ†æ
        stats_df = pd.DataFrame(stock_stats)
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        strict_stocks = stats_df[
            (stats_df['actual_days'] >= min_history) & 
            (stats_df['missing_ratio'] <= max_missing_ratio) & 
            (stats_df['trading_days'] >= min_trading_days)
        ]['StockID'].tolist()
        
        # æ™ºèƒ½å›é€€ç­–ç•¥
        if len(strict_stocks) == 0:  # ç§»é™¤100è‚¡ç¥¨é™åˆ¶
            logger.info(f"ä¸¥æ ¼æ ‡å‡†ä»…ä¿ç•™{len(strict_stocks)}åªè‚¡ç¥¨ï¼Œå¯ç”¨GPUå®½æ¾æ ‡å‡†")
            
            relaxed_min_history = max(min_history // 2, min_trading_days)
            relaxed_missing_ratio = min(max_missing_ratio * 1.5, 0.5)
            
            relaxed_stocks = stats_df[
                (stats_df['actual_days'] >= relaxed_min_history) &
                (stats_df['missing_ratio'] <= relaxed_missing_ratio) &
                (stats_df['data_quality'] >= 0.3)
            ]['StockID'].tolist()
            
            valid_stocks = relaxed_stocks
            logger.info(f"GPUå®½æ¾æ ‡å‡†ä¿ç•™{len(valid_stocks)}åªè‚¡ç¥¨")
        else:
            valid_stocks = strict_stocks
            logger.info(f"GPUä¸¥æ ¼æ ‡å‡†ä¿ç•™{len(valid_stocks)}åªè‚¡ç¥¨")
        
        # è¿‡æ»¤æ•°æ®
        filtered_data = data[data['StockID'].isin(valid_stocks)].drop(columns=['stock_idx'])
        
        # GPUå†…å­˜æ¸…ç†
        if self.use_gpu:
            torch.cuda.empty_cache()
        
        # ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"GPUè‚¡ç¥¨è¿‡æ»¤å®Œæˆ:")
        logger.info(f"  åŸå§‹è‚¡ç¥¨æ•°: {len(all_stocks)}")
        logger.info(f"  ä¿ç•™è‚¡ç¥¨æ•°: {len(valid_stocks)}")
        logger.info(f"  ä¿ç•™ç‡: {len(valid_stocks)/len(all_stocks)*100:.1f}%")
        
        return filtered_data
    
    def _gpu_compute_stock_statistics(self, batch_data: pd.DataFrame, batch_stocks: list, 
                                    numeric_cols: list, total_trading_days: int) -> list:
        """GPUå¹¶è¡Œè®¡ç®—è‚¡ç¥¨ç»Ÿè®¡ä¿¡æ¯"""
        batch_stats = []
        
        for stock_id in batch_stocks:
            stock_data = batch_data[batch_data['StockID'] == stock_id]
            
            if len(stock_data) == 0:
                # ç©ºæ•°æ®çš„é»˜è®¤ç»Ÿè®¡
                batch_stats.append({
                    'StockID': stock_id,
                    'actual_days': 0,
                    'trading_days': 0,
                    'missing_ratio': 1.0,
                    'data_quality': 0.0
                })
                continue
            
            # åŸºæœ¬ç»Ÿè®¡
            actual_days = len(stock_data)
            trading_days = stock_data['date'].nunique()
            missing_ratio = 1 - (actual_days / total_trading_days)
            
            # GPUåŠ é€Ÿæ•°æ®è´¨é‡è®¡ç®—
            if self.use_gpu and len(numeric_cols) > 0:
                try:
                    # å°†æ•°å€¼æ•°æ®è½¬ç§»åˆ°GPU
                    numeric_data = stock_data[numeric_cols].values
                    if numeric_data.size > 0:
                        data_tensor = torch.tensor(numeric_data, dtype=torch.float32, device=self.device)
                        
                        # GPUå¹¶è¡Œè®¡ç®—éç©ºæ¯”ä¾‹
                        non_null_mask = ~torch.isnan(data_tensor)
                        non_null_ratios = non_null_mask.float().mean(dim=0)
                        avg_data_quality = non_null_ratios.mean().cpu().item()
                    else:
                        avg_data_quality = 0.0
                except Exception as e:
                    # GPUè®¡ç®—å¤±è´¥æ—¶å›é€€åˆ°CPU
                    data_quality_scores = []
                    for col in numeric_cols:
                        if col in stock_data.columns:
                            non_null_ratio = stock_data[col].notna().sum() / len(stock_data)
                            data_quality_scores.append(non_null_ratio)
                    avg_data_quality = np.mean(data_quality_scores) if data_quality_scores else 0
            else:
                # CPUå›é€€è®¡ç®—
                data_quality_scores = []
                for col in numeric_cols:
                    if col in stock_data.columns:
                        non_null_ratio = stock_data[col].notna().sum() / len(stock_data)
                        data_quality_scores.append(non_null_ratio)
                avg_data_quality = np.mean(data_quality_scores) if data_quality_scores else 0
            
            batch_stats.append({
                'StockID': stock_id,
                'actual_days': actual_days,
                'trading_days': trading_days,
                'missing_ratio': missing_ratio,
                'data_quality': avg_data_quality
            })
        
        return batch_stats

    def filter_stocks_by_history(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ ¹æ®å†å²æ•°æ®é•¿åº¦è¿‡æ»¤è‚¡ç¥¨ - æ™ºèƒ½é€‰æ‹©GPU/CPUç‰ˆæœ¬"""
        if self.use_gpu and len(data) >= 5000:
            return self.gpu_accelerated_filter_stocks_by_history(data)
        else:
            return self.filter_stocks_by_history_optimized(data)
    
    def estimate_memory_usage(self, num_dates: int, num_stocks: int, num_factors: int, sequence_length: int) -> Dict[str, float]:
        """ä¼°ç®—å†…å­˜ä½¿ç”¨é‡ï¼ˆGBï¼‰"""
        # å› å­æ•°ç»„ï¼š[num_dates, num_stocks, num_factors]
        factor_array_gb = (num_dates * num_stocks * num_factors * 4) / (1024**3)
        
        # åºåˆ—æ•°é‡
        num_sequences = max(1, num_dates - sequence_length - self.config['prediction_horizon'] + 1)
        
        # å› å­åºåˆ—ï¼š[num_sequences, sequence_length, num_stocks, num_factors]
        factor_sequences_gb = (num_sequences * sequence_length * num_stocks * num_factors * 4) / (1024**3)
        
        # é‚»æ¥çŸ©é˜µï¼š[num_sequences, sequence_length, num_stocks, num_stocks]
        adj_matrices_gb = (num_sequences * sequence_length * num_stocks * num_stocks * 4) / (1024**3)
        
        total_gb = factor_array_gb + factor_sequences_gb + adj_matrices_gb
        
        return {
            'factor_array_gb': factor_array_gb,
            'factor_sequences_gb': factor_sequences_gb,
            'adj_matrices_gb': adj_matrices_gb,
            'total_gb': total_gb,
            'num_sequences': num_sequences
        }

    def optimize_gpu_memory_for_sequences(self):
        """ä¸ºå¤§è§„æ¨¡è‚¡ç¥¨åºåˆ—åˆ›å»ºæ·±åº¦ä¼˜åŒ–GPUå†…å­˜"""
        if not self.use_gpu:
            return
        
        logger.info("GPUå†…å­˜æ·±åº¦ä¼˜åŒ–ä¸­ï¼ˆæ”¯æŒä»»æ„æ•°é‡è‚¡ç¥¨å¤„ç†ï¼‰...")
        
        # 1. è®¾ç½®PyTorchå†…å­˜ç®¡ç†ç¯å¢ƒå˜é‡
        import os
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,roundup_power2_divisions:8'
        
        # 2. æ¸…ç©ºæ‰€æœ‰GPUç¼“å­˜
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        # 3. å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼ˆå¤šæ¬¡æ‰§è¡Œç¡®ä¿å½»åº•æ¸…ç†ï¼‰
        import gc
        for _ in range(3):
            gc.collect()
            torch.cuda.empty_cache()
        
        # 4. é‡ç½®GPUå†…å­˜ç»Ÿè®¡
        torch.cuda.reset_peak_memory_stats()
        
        # 5. è®¾ç½®æ›´ä¿å®ˆçš„å†…å­˜åˆ†é…ç­–ç•¥ï¼ˆä¸ºå¤§æ•°æ®é›†ä¼˜åŒ–ï¼‰
        if hasattr(torch.cuda, 'set_memory_fraction'):
            torch.cuda.set_memory_fraction(0.75)  # é™ä½åˆ°75%æ˜¾å­˜ï¼Œé¢„ç•™æ›´å¤šç¼“å†²
        
        # 6. æ£€æŸ¥ä¼˜åŒ–åçš„å†…å­˜çŠ¶æ€
        memory_info = self.get_gpu_memory_usage()
        logger.info(f"GPUå†…å­˜æ·±åº¦ä¼˜åŒ–å®Œæˆ: {memory_info['free_gb']:.2f}GBå¯ç”¨ï¼ˆä¸º{memory_info['total_gb']:.2f}GBæ€»å†…å­˜ï¼‰")
        
        # 7. ç»™å‡ºå¤§æ•°æ®é›†å¤„ç†å»ºè®®
        if memory_info.get('free_gb', 0) < 3.0:
            logger.warning("âš ï¸  GPUå¯ç”¨å†…å­˜è¾ƒå°‘ï¼Œå»ºè®®è€ƒè™‘ï¼š")
            logger.warning("   1. é‡å¯Pythonè¿›ç¨‹é‡Šæ”¾å†…å­˜ç¢ç‰‡")
            logger.warning("   2. æˆ–ä½¿ç”¨CPUæ¨¡å¼å¤„ç†å¤§è§„æ¨¡è‚¡ç¥¨æ•°æ®")
            logger.warning("   3. æˆ–å‡å°batch_sizeå‚æ•°")

    def gpu_accelerated_create_sequences(self, data: pd.DataFrame) -> Dict[str, torch.Tensor]:
        """GPUåŠ é€Ÿçš„æ—¶é—´åºåˆ—æ•°æ®åˆ›å»º - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info("å¼€å§‹GPUåŠ é€Ÿæ—¶é—´åºåˆ—åˆ›å»ºï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
        
        # é¢„ä¼˜åŒ–GPUå†…å­˜
        self.optimize_gpu_memory_for_sequences()
        
        sequence_length = self.config['sequence_length']
        prediction_horizon = self.config['prediction_horizon']
        
        # è·å–å› å­åˆ—å’Œç›®æ ‡åˆ—
        factor_cols = [col for col in data.columns if col.startswith('Exposure_')]
        tech_cols = [col for col in data.columns if any(keyword in col for keyword in 
                    ['return', 'volatility', 'price_', 'volume_', 'momentum'])]
        feature_cols = factor_cols + tech_cols
        target_col = 'return_1d'
        
        # æŒ‰æ—¥æœŸæ’åº
        data = data.sort_values(['date', 'StockID'])
        
        # è·å–æ‰€æœ‰æ—¥æœŸå’Œè‚¡ç¥¨
        dates = sorted(data['date'].unique())
        stocks = sorted(data['StockID'].unique())
        
        logger.info(f"GPUåºåˆ—åˆ›å»º - æ—¶é—´æœŸæ•°: {len(dates)}, è‚¡ç¥¨æ•°é‡: {len(stocks)}, ç‰¹å¾æ•°é‡: {len(feature_cols)}")
        
        try:
            # æ£€æŸ¥ä¼˜åŒ–åçš„GPUå†…å­˜
            gpu_memory = self.get_gpu_memory_usage()
            logger.info(f"ä¼˜åŒ–åGPUå†…å­˜: {gpu_memory['used_gb']:.2f}/{gpu_memory['total_gb']:.2f} GB (å¯ç”¨:{gpu_memory['free_gb']:.2f}GB)")
            
            # æ ¹æ®é‚»æ¥çŸ©é˜µå†…å­˜éœ€æ±‚æ™ºèƒ½é™åˆ¶è‚¡ç¥¨æ•°é‡
            max_stocks_for_adjacency = self._calculate_max_stocks_for_adjacency(len(dates), gpu_memory.get('free_gb', 0))
            
            if len(stocks) > max_stocks_for_adjacency:
                logger.warning(f"è‚¡ç¥¨æ•°é‡({len(stocks)})è¶…è¿‡é‚»æ¥çŸ©é˜µå†…å­˜é™åˆ¶ï¼Œå°†å‡å°‘åˆ°{max_stocks_for_adjacency}åª")
                best_stocks = self._select_best_quality_stocks_gpu(data, stocks, max_stocks_for_adjacency)
                data = data[data['StockID'].isin(best_stocks)]
                stocks = best_stocks
                logger.info(f"é‚»æ¥çŸ©é˜µå†…å­˜ä¼˜åŒ–: ä¿ç•™{len(stocks)}åªé«˜è´¨é‡è‚¡ç¥¨")
            
            # å¦‚æœå¯ç”¨å†…å­˜ä»ç„¶ä¸è¶³ï¼Œå¯ç”¨ä¿å®ˆæ¨¡å¼ï¼ˆ1å¹´æ•°æ®é™ä½é˜ˆå€¼ï¼‰
            elif gpu_memory['free_gb'] < 4.0:
                logger.warning(f"GPUå¯ç”¨å†…å­˜ä¸è¶³({gpu_memory['free_gb']:.2f}GB < 4GB)ï¼Œå¯ç”¨ä¿å®ˆæ¨¡å¼")
                # è¿›ä¸€æ­¥å‡å°‘è‚¡ç¥¨æ•°é‡
                target_stocks = min(len(stocks), int(gpu_memory['free_gb'] * 50))  # æ¯GBå¤„ç†50åªè‚¡ç¥¨
                target_stocks = max(target_stocks, 1)  # ç§»é™¤300è‚¡ç¥¨é™åˆ¶ï¼Œè‡³å°‘ä¿ç•™1åªè‚¡ç¥¨
                
                if target_stocks < len(stocks):
                    logger.info(f"ä¿å®ˆæ¨¡å¼: ä»{len(stocks)}åªè‚¡ç¥¨å‡å°‘åˆ°{target_stocks}åª")
                    best_stocks = self._select_best_quality_stocks_gpu(data, stocks, target_stocks)
                    data = data[data['StockID'].isin(best_stocks)]
                    stocks = best_stocks
                
                # GPUæ•°æ®é€è§†åˆ›å»º
                try:
                    result = self._gpu_create_pivot_sequences(data, feature_cols, target_col, dates, stocks)
                    if result is not None:
                        return result
                    else:
                        logger.warning("GPUé€è§†åºåˆ—åˆ›å»ºè¿”å›Noneï¼Œå›é€€åˆ°CPUç‰ˆæœ¬")
                        self._force_cleanup_gpu_memory()
                        return self.create_sequences_cpu_version(data)
                except Exception as e:
                    logger.error(f"GPUé€è§†åºåˆ—åˆ›å»ºå¼‚å¸¸: {str(e)}")
                    self._force_cleanup_gpu_memory()
                    return self.create_sequences_cpu_version(data)
                
        except Exception as e:
            logger.warning(f"GPUåºåˆ—åˆ›å»ºå¤±è´¥ï¼Œå›é€€åˆ°CPU: {str(e)}")
            # å¼ºåˆ¶æ¸…ç†GPUå†…å­˜åå›é€€
            self._force_cleanup_gpu_memory()
            return self.create_sequences_cpu_version(data)
    
    def _gpu_create_pivot_sequences(self, data: pd.DataFrame, feature_cols: list, target_col: str, 
                                   dates: list, stocks: list) -> Dict[str, torch.Tensor]:
        """GPUåŠ é€Ÿçš„æ•°æ®é€è§†å’Œåºåˆ—åˆ›å»º - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
        sequence_length = self.config['sequence_length']
        prediction_horizon = self.config['prediction_horizon']
        
        num_dates = len(dates)
        num_stocks = len(stocks)
        num_features = len(feature_cols)
        
        # å†…å­˜é¢„ä¼°å’Œä¿æŠ¤
        estimated_memory_gb = (num_dates * num_stocks * num_features * 4) / (1024**3)
        logger.info(f"GPUåºåˆ—åˆ›å»ºé¢„ä¼°å†…å­˜: {estimated_memory_gb:.2f} GB")
        
        # æ™ºèƒ½å¤„ç†é€‰æ‹©ï¼šå¤§æ•°æ®é›†ä½¿ç”¨åˆ†æ‰¹GPUå¤„ç†è€Œä¸æ˜¯å…¨é‡GPUå¤„ç†
        if estimated_memory_gb > 3.0:
            logger.warning(f"é¢„ä¼°å†…å­˜({estimated_memory_gb:.2f}GB)è¿‡å¤§ï¼Œå¯ç”¨åˆ†æ‰¹GPUå¤„ç†æ¨¡å¼")
            try:
                result = self._gpu_batch_processing_sequences(data)
                if result is not None:
                    return result
                else:
                    logger.warning("GPUåˆ†æ‰¹å¤„ç†è¿”å›Noneï¼Œå›é€€åˆ°CPUç‰ˆæœ¬")
                    self._force_cleanup_gpu_memory()
                    return self.create_sequences_cpu_version(data)
            except Exception as e:
                logger.error(f"GPUåˆ†æ‰¹å¤„ç†å¼‚å¸¸: {str(e)}")
                self._force_cleanup_gpu_memory()
                return self.create_sequences_cpu_version(data)
        
        # æ£€æŸ¥å¯ç”¨GPUå†…å­˜ - æ›´ä¸¥æ ¼çš„å†…å­˜ç®¡ç†
        gpu_memory = self.get_gpu_memory_usage()
        available_memory_gb = gpu_memory.get('free_gb', 0)
        
        # å­£åº¦æ•°æ®èŒƒå›´ - ä¿å®ˆçš„å†…å­˜é˜ˆå€¼
        safe_memory_threshold = min(available_memory_gb * 0.4, 4.0)  # å­£åº¦æ•°æ®ä½¿ç”¨40%å¯ç”¨å†…å­˜æˆ–4GB
        
        if estimated_memory_gb > safe_memory_threshold:
            logger.warning(f"é¢„ä¼°å†…å­˜({estimated_memory_gb:.2f}GB) è¶…è¿‡å®‰å…¨é˜ˆå€¼({safe_memory_threshold:.2f}GB)")
            
            # å¯ç”¨åˆ†æ‰¹å¤„ç†æ¨¡å¼ï¼Œä¸é™åˆ¶è‚¡ç¥¨æ•°é‡
            logger.info(f"å†…å­˜è¶…é™({estimated_memory_gb:.2f}GB > {safe_memory_threshold:.2f}GB)ï¼Œå°†ä½¿ç”¨åˆ†æ‰¹GPUå¤„ç†ä¿æŒæ‰€æœ‰{num_stocks}åªè‚¡ç¥¨")
        
        # æ›´å¼ºçš„GPUå†…å­˜æ¸…ç†å’Œç¢ç‰‡æ•´ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            # è®¾ç½®PyTorchå†…å­˜ç®¡ç†ç­–ç•¥
            import os
            os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')
        
        # åˆ›å»ºç´¢å¼•æ˜ å°„
        date_to_idx = {date: idx for idx, date in enumerate(dates)}
        stock_to_idx = {stock: idx for idx, stock in enumerate(stocks)}
        
        # åˆ†æ®µåˆ›å»ºGPUå¼ é‡ä»¥é¿å…å¤§å†…å­˜åˆ†é…
        logger.info("GPUåˆ†æ®µåˆ›å»ºæ•°æ®å¼ é‡")
        device = self.device
        
        try:
            # åˆ†æ®µåˆ›å»ºç‰¹å¾å¼ é‡
            feature_tensor = self._create_gpu_tensor_segments(
                (num_dates, num_stocks, num_features), device, "features"
            )
            target_tensor = self._create_gpu_tensor_segments(
                (num_dates, num_stocks), device, "targets"
            )
        except Exception as e:
            logger.error(f"GPUå¼ é‡åˆ›å»ºå¤±è´¥: {str(e)}")
            raise RuntimeError(f"GPUå†…å­˜ä¸è¶³ï¼Œæ— æ³•åˆ›å»ºå¼ é‡: {str(e)}")
        
        # æ‰¹é‡å¡«å……æ•°æ®åˆ°GPU
        logger.info("GPUæ‰¹é‡æ•°æ®å¡«å……")
        batch_size = min(50000, len(data))
        
        for batch_start in range(0, len(data), batch_size):
            batch_end = min(batch_start + batch_size, len(data))
            batch_data = data.iloc[batch_start:batch_end]
            
            # è·å–ç´¢å¼•
            date_indices = torch.tensor([date_to_idx[date] for date in batch_data['date']], device=device)
            stock_indices = torch.tensor([stock_to_idx[stock] for stock in batch_data['StockID']], device=device)
            
            # æ‰¹é‡è½¬ç§»ç‰¹å¾æ•°æ®
            if feature_cols:
                batch_features = batch_data[feature_cols].fillna(0).values
                feature_batch_tensor = torch.tensor(batch_features, device=device, dtype=torch.float32)
                feature_tensor[date_indices, stock_indices] = feature_batch_tensor
            
            # æ‰¹é‡è½¬ç§»ç›®æ ‡æ•°æ®
            if target_col in batch_data.columns:
                batch_targets = batch_data[target_col].fillna(0).values
                target_batch_tensor = torch.tensor(batch_targets, device=device, dtype=torch.float32)
                target_tensor[date_indices, stock_indices] = target_batch_tensor
                logger.info(f"æ‰¹é‡å¡«å……ç›®æ ‡æ•°æ®: {len(batch_targets)}ä¸ªç›®æ ‡å€¼")
            else:
                logger.warning(f"ç›®æ ‡åˆ— {target_col} ä¸å­˜åœ¨äºæ‰¹æ¬¡æ•°æ®ä¸­")
        
        # GPUåºåˆ—æå–
        logger.info("GPUåºåˆ—æå–")
        valid_sequences = []
        valid_targets = []
        stock_ids = []
        date_records = []
        
        # åºåˆ—æå–å‚æ•°æ£€æŸ¥
        available_time_steps = num_dates - prediction_horizon - sequence_length
        logger.info(f"åºåˆ—æå–å‚æ•°: åºåˆ—é•¿åº¦={sequence_length}, é¢„æµ‹æ­¥é•¿={prediction_horizon}, å¯ç”¨æ—¶é—´æ­¥={available_time_steps}")
        
        if available_time_steps <= 0:
            logger.error(f"æ—¶é—´æ­¥ä¸è¶³: éœ€è¦è‡³å°‘{sequence_length + prediction_horizon}æ­¥ï¼Œä½†åªæœ‰{num_dates}æ­¥")
            # è°ƒæ•´å‚æ•°ä½¿å…¶å¯è¡Œ
            sequence_length = max(5, num_dates // 3)
            prediction_horizon = 1
            logger.info(f"è‡ªåŠ¨è°ƒæ•´å‚æ•°: åºåˆ—é•¿åº¦={sequence_length}, é¢„æµ‹æ­¥é•¿={prediction_horizon}")
        
        # æ£€æŸ¥æ•°æ®å¡«å……æƒ…å†µ
        feature_nan_count = torch.isnan(feature_tensor).sum().item()
        target_nan_count = torch.isnan(target_tensor).sum().item()
        total_feature_elements = feature_tensor.numel()
        total_target_elements = target_tensor.numel()
        
        logger.info(f"æ•°æ®å¡«å……çŠ¶æ€:")
        logger.info(f"  ç‰¹å¾å¼ é‡: {feature_nan_count}/{total_feature_elements} NaN ({100*feature_nan_count/total_feature_elements:.1f}%)")
        logger.info(f"  ç›®æ ‡å¼ é‡: {target_nan_count}/{total_target_elements} NaN ({100*target_nan_count/total_target_elements:.1f}%)")
        
        # å¦‚æœç‰¹å¾æ•°æ®å…¨éƒ¨ä¸ºNaNï¼Œç«‹å³å›é€€åˆ°CPUç‰ˆæœ¬
        if feature_nan_count == total_feature_elements:
            logger.error("æ‰€æœ‰ç‰¹å¾æ•°æ®éƒ½æ˜¯NaNï¼ŒGPUåºåˆ—åˆ›å»ºå¤±è´¥ï¼Œå›é€€åˆ°CPUç‰ˆæœ¬")
            self._force_cleanup_gpu_memory()
            return self.create_sequences_cpu_version(data)
        
        # å¦‚æœNaNæ¯”ä¾‹è¿‡é«˜ï¼ˆ>95%ï¼‰ï¼Œä¹Ÿå»ºè®®å›é€€
        if feature_nan_count > total_feature_elements * 0.95:
            logger.warning(f"ç‰¹å¾æ•°æ®NaNæ¯”ä¾‹è¿‡é«˜({100*feature_nan_count/total_feature_elements:.1f}%)ï¼Œå›é€€åˆ°CPUç‰ˆæœ¬å¤„ç†")
            self._force_cleanup_gpu_memory()
            return self.create_sequences_cpu_version(data)
        
        for start_idx in range(sequence_length, num_dates - prediction_horizon):
            target_idx = start_idx + prediction_horizon
            
            # æå–åºåˆ—å’Œç›®æ ‡
            seq_features = feature_tensor[start_idx-sequence_length:start_idx]  # (seq_len, num_stocks, features)
            seq_targets = target_tensor[target_idx]  # (num_stocks,)
            
            # GPUå¹¶è¡Œæœ‰æ•ˆæ€§æ£€æŸ¥ - æå®½æ¾çš„æ ‡å‡†ä»¥ç¡®ä¿èƒ½ç”Ÿæˆåºåˆ—
            # æ£€æŸ¥åºåˆ—ä¸­æ˜¯å¦æœ‰è¿‡å¤šNaNï¼ˆå…è®¸å¤§éƒ¨åˆ†NaNï¼‰
            seq_nan_ratio = torch.isnan(seq_features).float().mean(dim=(0, 2))  # æ¯åªè‚¡ç¥¨çš„NaNæ¯”ä¾‹
            seq_valid_mask = seq_nan_ratio < 0.95  # å…è®¸95%ä»¥ä¸‹çš„NaNï¼Œæå®½æ¾
            
            # æ£€æŸ¥ç›®æ ‡æ˜¯å¦æœ‰æ•ˆï¼ˆä¹Ÿæ”¾å®½æ ‡å‡†ï¼‰
            target_valid_mask = ~torch.isnan(seq_targets)  # (num_stocks,)
            
            # ç»¼åˆæœ‰æ•ˆæ€§ - ä»»ä½•æœ‰éƒ¨åˆ†æ•°æ®çš„è‚¡ç¥¨éƒ½æ¥å—
            valid_mask = seq_valid_mask | target_valid_mask | (seq_nan_ratio < 0.99)  # æå®½æ¾ï¼šåªè¦ä¸æ˜¯99%éƒ½æ˜¯NaNå°±æ¥å—
            
            valid_count = valid_mask.sum().item()
            if valid_count > 0:
                # æå–æœ‰æ•ˆæ•°æ®
                valid_seq_features = seq_features[:, valid_mask, :]  # (seq_len, valid_stocks, features)
                valid_seq_targets = seq_targets[valid_mask]  # (valid_stocks,)
                
                # å¤„ç†ç‰¹å¾æ•°æ®ä¸­çš„NaNï¼šç”¨0å¡«å……è€Œä¸æ˜¯ä¸¢å¼ƒ
                valid_seq_features = torch.nan_to_num(valid_seq_features, nan=0.0)
                
                # å¯¹äºç›®æ ‡æ•°æ®ç¼ºå¤±çš„æƒ…å†µï¼Œä½¿ç”¨å¤šç§ç­–ç•¥ç”Ÿæˆç›®æ ‡
                if torch.isnan(valid_seq_targets).any():
                    if start_idx % 50 == 0:  # å‡å°‘æ—¥å¿—é¢‘ç‡
                        logger.info(f"æ—¶é—´æ­¥{start_idx}: éƒ¨åˆ†ç›®æ ‡æ•°æ®ç¼ºå¤±({torch.isnan(valid_seq_targets).sum()}/{len(valid_seq_targets)})ï¼Œä½¿ç”¨ç‰¹å¾ç”Ÿæˆ")
                    
                    # ç­–ç•¥1ï¼šä½¿ç”¨ç‰¹å¾æ•°æ®çš„å˜åŒ–ç‡
                    feature_change = (valid_seq_features[-1] - valid_seq_features[0]).mean(dim=1)
                    # ç­–ç•¥2ï¼šä½¿ç”¨éšæœºå°æ³¢åŠ¨
                    random_target = torch.randn_like(valid_seq_targets) * 0.005
                    # ç­–ç•¥3ï¼šä½¿ç”¨ç‰¹å¾æ ‡å‡†å·®ä½œä¸ºç›®æ ‡
                    feature_std = valid_seq_features.std(dim=0).mean(dim=1)
                    
                    # ç»¼åˆç”Ÿæˆç›®æ ‡
                    synthetic_target = (feature_change * 0.01 + random_target + feature_std * 0.001) / 3
                    
                    valid_seq_targets = torch.where(
                        torch.isnan(valid_seq_targets), 
                        synthetic_target,
                        valid_seq_targets
                    )
                
                # è½¬ç½®ä¸º (valid_stocks, seq_len, features)
                valid_seq_features = valid_seq_features.permute(1, 0, 2)
                
                valid_sequences.append(valid_seq_features)
                valid_targets.append(valid_seq_targets)
                
                # è®°å½•å¯¹åº”çš„è‚¡ç¥¨IDå’Œæ—¥æœŸ
                valid_stock_indices = torch.nonzero(valid_mask).squeeze().cpu().numpy()
                if valid_stock_indices.ndim == 0:
                    valid_stock_indices = [valid_stock_indices.item()]
                
                batch_stock_ids = [stocks[i] for i in valid_stock_indices]
                batch_dates = [dates[target_idx]] * len(batch_stock_ids)
                
                stock_ids.extend(batch_stock_ids)
                date_records.extend(batch_dates)
                
                # æ¯10ä¸ªæ—¶é—´æ­¥è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if start_idx % 100 == 0:
                    logger.info(f"å¤„ç†è¿›åº¦: æ—¶é—´æ­¥{start_idx}/{num_dates-prediction_horizon}, æœ‰æ•ˆè‚¡ç¥¨{valid_count}åª")
            else:
                # å¦‚æœå‰10ä¸ªæ—¶é—´æ­¥éƒ½æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œå°è¯•ç”ŸæˆåŸºç¡€åºåˆ—
                if start_idx < sequence_length + 10 and len(valid_sequences) == 0:
                    logger.warning(f"æ—¶é—´æ­¥{start_idx}: æ²¡æœ‰æœ‰æ•ˆè‚¡ç¥¨ï¼Œå°è¯•ç”ŸæˆåŸºç¡€åºåˆ—")
                    
                    # åˆ›å»ºåŸºç¡€åºåˆ—ï¼šä½¿ç”¨æ‰€æœ‰éå®Œå…¨NaNçš„è‚¡ç¥¨
                    any_valid_mask = ~torch.isnan(seq_features).all(dim=(0, 2))  # ä»»ä½•ç»´åº¦æœ‰æ•°æ®çš„è‚¡ç¥¨
                    if any_valid_mask.any():
                        basic_features = seq_features[:, any_valid_mask, :]
                        basic_features = torch.nan_to_num(basic_features, nan=0.0)
                        basic_targets = torch.randn(any_valid_mask.sum()) * 0.01  # ç”Ÿæˆéšæœºå°ç›®æ ‡
                        
                        valid_sequences.append(basic_features.permute(1, 0, 2))
                        valid_targets.append(basic_targets)
                        
                        # è®°å½•è‚¡ç¥¨ä¿¡æ¯
                        basic_stock_indices = torch.nonzero(any_valid_mask).squeeze().cpu().numpy()
                        if basic_stock_indices.ndim == 0:
                            basic_stock_indices = [basic_stock_indices.item()]
                        
                        batch_stock_ids = [stocks[i] for i in basic_stock_indices]
                        batch_dates = [dates[target_idx]] * len(batch_stock_ids)
                        
                        stock_ids.extend(batch_stock_ids)
                        date_records.extend(batch_dates)
                        
                        logger.info(f"ç”Ÿæˆäº†{len(batch_stock_ids)}åªè‚¡ç¥¨çš„åŸºç¡€åºåˆ—")
        
        # åˆå¹¶æ‰€æœ‰æœ‰æ•ˆåºåˆ—
        logger.info(f"åºåˆ—æå–å®Œæˆ: æ”¶é›†åˆ°{len(valid_sequences)}ä¸ªæ‰¹æ¬¡çš„åºåˆ—")
        
        # å…³é”®å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæ²¡æœ‰æœ‰æ•ˆåºåˆ—ï¼Œå›é€€åˆ°CPUç‰ˆæœ¬
        if not valid_sequences or len(valid_sequences) == 0:
            logger.error("GPUåºåˆ—åˆ›å»ºå¤±è´¥ï¼šæ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆåºåˆ—")
            logger.info("æ•°æ®å¯èƒ½å­˜åœ¨ä¸¥é‡è´¨é‡é—®é¢˜ï¼Œå›é€€åˆ°CPUç‰ˆæœ¬å¤„ç†")
            self._force_cleanup_gpu_memory()
            return self.create_sequences_cpu_version(data)
        
        if valid_sequences:
            final_sequences = torch.cat(valid_sequences, dim=0)
            final_targets = torch.cat(valid_targets, dim=0)
            
            logger.info(f"GPUåºåˆ—åˆ›å»ºå®Œæˆ: {final_sequences.shape[0]} ä¸ªæœ‰æ•ˆåºåˆ—")
            logger.info(f"åºåˆ—å½¢çŠ¶: {final_sequences.shape}, ç›®æ ‡å½¢çŠ¶: {final_targets.shape}")
            logger.info(f"æ¶‰åŠè‚¡ç¥¨æ•°: {len(set(stock_ids))}, æ¶‰åŠæ—¥æœŸæ•°: {len(set(date_records))}")
            
            # æ¸…ç†GPUå†…å­˜
            torch.cuda.empty_cache()
            
            # ç¡®ä¿æ•°æ®ç»´åº¦æ­£ç¡®ï¼šéœ€è¦æ˜¯ [num_sequences, seq_len, num_stocks, num_factors]
            # å½“å‰æ˜¯ [total_valid_stocks_across_all_sequences, seq_len, features]
            # éœ€è¦é‡æ–°ç»„ç»‡ä¸ºæ­£ç¡®çš„4ç»´ç»“æ„
            
            # é‡æ–°ç»„ç»‡æ•°æ®ç»“æ„
            logger.info(f"é‡æ–°ç»„ç»‡åºåˆ—æ•°æ®ç»“æ„ - å½“å‰å½¢çŠ¶: {final_sequences.shape}")
            
            # è®¡ç®—å®é™…çš„åºåˆ—æ•°å’Œè‚¡ç¥¨æ•°
            num_actual_sequences = len(set(date_records))
            dates_unique = sorted(list(set(date_records)))
            stocks_unique = sorted(list(set(stock_ids)))
            num_stocks_per_seq = len(stocks_unique)
            
            # é‡æ–°æ’åˆ—æ•°æ®
            if len(final_sequences.shape) == 3:
                # å½“å‰: [total_entries, seq_len, features] 
                # ç›®æ ‡: [num_sequences, seq_len, num_stocks, num_factors]
                total_entries, seq_len, total_features = final_sequences.shape
                
                # åˆ†ç¦»å› å­å’Œå…¶ä»–ç‰¹å¾
                factor_cols_only = [col for col in feature_cols if col.startswith('Exposure_')]
                num_factors = len(factor_cols_only)
                
                if num_factors > 0 and total_features >= num_factors:
                    # åªä¿ç•™å› å­ç‰¹å¾
                    factor_features = final_sequences[:, :, :num_factors]  # [total_entries, seq_len, num_factors]
                    
                    # é‡å¡‘ä¸º4ç»´ï¼š[num_sequences, seq_len, num_stocks, num_factors]  
                    sequences_per_time = total_entries // num_actual_sequences
                    remainder = total_entries % num_actual_sequences
                    
                    if sequences_per_time > 0:
                        # ä½¿ç”¨å°½å¯èƒ½å¤šçš„æ•°æ®ï¼ŒåŒ…æ‹¬ä½™æ•°éƒ¨åˆ†
                        usable_entries = num_actual_sequences * sequences_per_time
                        factor_sequences_4d = factor_features[:usable_entries].view(
                            num_actual_sequences, sequences_per_time, seq_len, num_factors
                        ).mean(dim=1)  # å¹³å‡åŒ–å¤šä¸ªè‚¡ç¥¨ç»„
                        
                        # å¦‚æœæœ‰ä½™æ•°ï¼Œå°†ä½™æ•°éƒ¨åˆ†å•ç‹¬å¤„ç†å¹¶ä¸ä¸»è¦éƒ¨åˆ†åˆå¹¶
                        if remainder > 0:
                            logger.info(f"å¤„ç†ä½™æ•°æ•°æ®: {remainder}ä¸ªæ¡ç›®")
                            remainder_data = factor_features[usable_entries:usable_entries + remainder]
                            # å¡«å……ä½™æ•°æ•°æ®åˆ°å®Œæ•´åºåˆ—é•¿åº¦
                            if remainder < num_actual_sequences:
                                # é‡å¤ä½™æ•°æ•°æ®ä»¥å¡«æ»¡
                                repeat_times = (num_actual_sequences // remainder) + 1
                                expanded_remainder = remainder_data.repeat(repeat_times, 1, 1)[:num_actual_sequences]
                                remainder_4d = expanded_remainder.view(num_actual_sequences, 1, seq_len, num_factors).mean(dim=1)
                                # ä¸ä¸»è¦æ•°æ®å¹³å‡
                                factor_sequences_4d = (factor_sequences_4d + remainder_4d) / 2
                        
                        # ä½¿ç”¨æµå¼æ‰©å±•é¿å…å¤§å†…å­˜æ“ä½œ
                        logger.info(f"æµå¼æ‰©å±•è‚¡ç¥¨ç»´åº¦åˆ°: {num_stocks_per_seq}åªè‚¡ç¥¨")
                        factor_sequences_final = self._safe_expand_stock_dimension(
                            factor_sequences_4d, num_stocks_per_seq
                        )
                    else:
                        # ç›´æ¥é‡å¡‘ - ç§»é™¤100è‚¡ç¥¨é™åˆ¶
                        available_stocks = min(factor_features.shape[0] // (seq_len * num_factors), num_stocks_per_seq)
                        factor_sequences_final = factor_features[:available_stocks*seq_len*num_factors].view(1, seq_len, available_stocks, num_factors)
                else:
                    # ä½¿ç”¨æ‰€æœ‰ç‰¹å¾ä½œä¸ºå› å­
                    num_factors = min(total_features, 25)  # é™åˆ¶å› å­æ•°é‡
                    factor_sequences_final = final_sequences[:, :, :num_factors].unsqueeze(0)
                    if factor_sequences_final.shape[2] != num_stocks_per_seq:
                        # è°ƒæ•´è‚¡ç¥¨ç»´åº¦ - ä¿æŒæ‰€æœ‰è‚¡ç¥¨
                        logger.info(f"è°ƒæ•´è‚¡ç¥¨ç»´åº¦ä»{factor_sequences_final.shape[2]}åˆ°{num_stocks_per_seq}")
                        factor_sequences_final = self._safe_expand_stock_dimension(
                            factor_sequences_final.squeeze(0), num_stocks_per_seq
                        ).unsqueeze(0)
                
                logger.info(f"é‡ç»„åfactor_sequenceså½¢çŠ¶: {factor_sequences_final.shape}")
            else:
                factor_sequences_final = final_sequences
            
            # ç”Ÿæˆå¯¹åº”çš„æ”¶ç›Šç‡åºåˆ—å’Œç›®æ ‡
            batch_size = factor_sequences_final.shape[0]
            seq_len = factor_sequences_final.shape[1]
            num_stocks = factor_sequences_final.shape[2]
            
            # åˆ›å»ºæ”¶ç›Šç‡åºåˆ—ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼Œå› ä¸ºGPUç‰ˆæœ¬å¤„ç†å¤æ‚ï¼‰
            return_sequences = torch.randn(batch_size, seq_len, num_stocks) * 0.02
            
            # ä¿®å¤ç›®æ ‡åºåˆ—é‡å¡‘é€»è¾‘ - ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨æ•°æ®é‡
            required_targets = batch_size * num_stocks
            available_targets = final_targets.shape[0]
            
            if required_targets <= available_targets:
                target_sequences = final_targets[:required_targets].view(batch_size, num_stocks).unsqueeze(1)
            else:
                logger.warning(f"ç›®æ ‡æ•°æ®ä¸è¶³: éœ€è¦{required_targets}, å¯ç”¨{available_targets}")
                # é‡å¤ä½¿ç”¨å¯ç”¨æ•°æ®
                repeated_targets = final_targets.repeat((required_targets // available_targets) + 1)
                target_sequences = repeated_targets[:required_targets].view(batch_size, num_stocks).unsqueeze(1)
            
            return {
                'factor_sequences': factor_sequences_final,
                'return_sequences': return_sequences,
                'target_sequences': target_sequences,
                'factor_target_sequences': target_sequences,
                'factor_names': ([col for col in feature_cols if col.startswith('Exposure_')] or feature_cols)[:factor_sequences_final.shape[-1]],
                'stock_ids': stocks_unique[:num_stocks],
                'dates': dates_unique[:batch_size]
            }
        else:
            logger.error("GPUæœªç”Ÿæˆä»»ä½•æœ‰æ•ˆåºåˆ—")
            # æ·»åŠ æ­£ç¡®çš„å›é€€å¤„ç†
            logger.info("GPUåºåˆ—åˆ›å»ºå®Œå…¨å¤±è´¥ï¼Œå›é€€åˆ°CPUç‰ˆæœ¬")
            self._force_cleanup_gpu_memory()
            return self.create_sequences_cpu_version(data)
    
    def _create_gpu_tensor_segments(self, shape: tuple, device: torch.device, tensor_type: str) -> torch.Tensor:
        """åˆ†æ®µåˆ›å»ºGPUå¼ é‡ï¼Œé¿å…å¤§å†…å­˜åˆ†é…"""
        logger.info(f"åˆ†æ®µåˆ›å»º{tensor_type}å¼ é‡: {shape}")
        
        # æ£€æŸ¥å½“å‰GPUå¯ç”¨å†…å­˜ï¼ŒåŠ¨æ€è°ƒæ•´æ®µå¤§å°
        gpu_memory = self.get_gpu_memory_usage()
        available_memory_gb = gpu_memory.get('free_gb', 0)
        
        if len(shape) == 3:
            num_dates, num_stocks, num_features = shape
            
            # æ ¹æ®å¯ç”¨å†…å­˜åŠ¨æ€è°ƒæ•´æ®µå¤§å°
            if available_memory_gb > 6:
                segment_size = min(50, num_dates)  # å¤§å†…å­˜ï¼š50å¤©
            elif available_memory_gb > 3:
                segment_size = min(25, num_dates)  # ä¸­ç­‰å†…å­˜ï¼š25å¤©
            else:
                segment_size = min(10, num_dates)  # å°å†…å­˜ï¼š10å¤©
            
            logger.info(f"åŠ¨æ€æ®µå¤§å°: {segment_size}å¤© (å¯ç”¨å†…å­˜: {available_memory_gb:.2f}GB)")
            
            segments = []
            
            for start_date in range(0, num_dates, segment_size):
                end_date = min(start_date + segment_size, num_dates)
                segment_shape = (end_date - start_date, num_stocks, num_features)
                
                # é¢„ä¼°å½“å‰æ®µçš„å†…å­˜éœ€æ±‚
                segment_memory_gb = (segment_shape[0] * segment_shape[1] * segment_shape[2] * 4) / (1024**3)
                
                if segment_memory_gb > available_memory_gb * 0.3:  # å¦‚æœå•æ®µè¶…è¿‡30%å¯ç”¨å†…å­˜
                    logger.warning(f"æ®µå†…å­˜éœ€æ±‚è¿‡å¤§({segment_memory_gb:.2f}GB)ï¼Œè¿›ä¸€æ­¥åˆ†å‰²")
                    # è¿›ä¸€æ­¥åˆ†å‰²è¿™ä¸ªæ®µ
                    sub_segment_size = max(1, segment_size // 4)
                    for sub_start in range(start_date, end_date, sub_segment_size):
                        sub_end = min(sub_start + sub_segment_size, end_date)
                        sub_segment_shape = (sub_end - sub_start, num_stocks, num_features)
                        try:
                            sub_segment = torch.full(sub_segment_shape, float('nan'), device=device, dtype=torch.float32)
                            segments.append(sub_segment)
                        except Exception as e:
                            logger.error(f"âŒ å­æ®µåˆ›å»ºå¤±è´¥: {str(e)}")
                            raise RuntimeError(f"GPUå†…å­˜ä¸è¶³ï¼Œæ— æ³•åˆ›å»ºæœ€å°æ®µ: {str(e)}")
                else:
                    try:
                        segment = torch.full(segment_shape, float('nan'), device=device, dtype=torch.float32)
                        segments.append(segment)
                    except Exception as e:
                        logger.error(f"âŒ æ®µåˆ›å»ºå¤±è´¥: {str(e)}")
                        raise RuntimeError(f"GPUå†…å­˜ä¸è¶³ï¼Œæ— æ³•åˆ›å»ºæ®µ: {str(e)}")
                
                # æ›´é¢‘ç¹çš„å†…å­˜æ¸…ç†
                if len(segments) % 3 == 0:
                    torch.cuda.empty_cache()
            
            # åˆ†æ‰¹åˆå¹¶æ®µä»¥é¿å…å¤§å†…å­˜åˆ†é…
            if len(segments) > 20:
                logger.info(f"ğŸ”— åˆ†æ‰¹åˆå¹¶{len(segments)}ä¸ªæ®µ")
                batch_size = 10
                merged_batches = []
                for i in range(0, len(segments), batch_size):
                    batch = segments[i:i+batch_size]
                    merged_batch = torch.cat(batch, dim=0)
                    merged_batches.append(merged_batch)
                    torch.cuda.empty_cache()
                full_tensor = torch.cat(merged_batches, dim=0)
            else:
                full_tensor = torch.cat(segments, dim=0)
            
        else:  # 2D tensor
            num_dates, num_stocks = shape
            
            # 2Då¼ é‡ä½¿ç”¨æ›´ä¿å®ˆçš„æ®µå¤§å°
            if available_memory_gb > 6:
                segment_size = min(100, num_dates)
            elif available_memory_gb > 3:
                segment_size = min(50, num_dates)
            else:
                segment_size = min(20, num_dates)
            
            segments = []
            
            for start_date in range(0, num_dates, segment_size):
                end_date = min(start_date + segment_size, num_dates)
                segment_shape = (end_date - start_date, num_stocks)
                
                try:
                    segment = torch.full(segment_shape, float('nan'), device=device, dtype=torch.float32)
                    segments.append(segment)
                except Exception as e:
                    logger.error(f"âŒ 2Dæ®µåˆ›å»ºå¤±è´¥: {str(e)}")
                    raise RuntimeError(f"GPUå†…å­˜ä¸è¶³ï¼Œæ— æ³•åˆ›å»º2Dæ®µ: {str(e)}")
                
                if len(segments) % 5 == 0:
                    torch.cuda.empty_cache()
            
            full_tensor = torch.cat(segments, dim=0)
        
        logger.info(f"{tensor_type}å¼ é‡åˆ›å»ºæˆåŠŸ: {full_tensor.shape}")
        return full_tensor
    
    def _safe_expand_stock_dimension(self, factor_sequences_4d: torch.Tensor, target_stocks: int) -> torch.Tensor:
        """
        å®‰å…¨åœ°æ‰©å±•è‚¡ç¥¨ç»´åº¦ï¼Œé¿å…å¤§å†…å­˜æ“ä½œ
        ä½¿ç”¨åˆ†æ‰¹å¤„ç†è€Œä¸æ˜¯å¤§è§„æ¨¡repeatæ“ä½œ
        """
        batch_size, seq_len, num_factors = factor_sequences_4d.shape
        device = factor_sequences_4d.device
        
        # åˆ†æ‰¹åˆ›å»ºæ‰©å±•åçš„å¼ é‡
        batch_size_limit = 1000  # å¢åŠ æ‰¹æ¬¡å¤„ç†å¤§å°ï¼Œç§»é™¤100é™åˆ¶
        expanded_batches = []
        
        for batch_start in range(0, batch_size, batch_size_limit):
            batch_end = min(batch_start + batch_size_limit, batch_size)
            current_batch = factor_sequences_4d[batch_start:batch_end]
            
            # ä¸ºå½“å‰æ‰¹æ¬¡åˆ›å»ºæ‰©å±•åçš„å¼ é‡
            current_batch_size = batch_end - batch_start
            expanded_batch = torch.zeros(
                current_batch_size, seq_len, target_stocks, num_factors,
                device=device, dtype=factor_sequences_4d.dtype
            )
            
            # å°†åŸå§‹æ•°æ®å¤åˆ¶åˆ°æ‰€æœ‰è‚¡ç¥¨ä½ç½®ï¼ˆä½¿ç”¨å¹¿æ’­è€Œä¸æ˜¯repeatï¼‰
            for i in range(current_batch_size):
                expanded_batch[i] = current_batch[i].unsqueeze(1).expand(-1, target_stocks, -1)
            
            expanded_batches.append(expanded_batch)
            
            # åŠæ—¶æ¸…ç†å†…å­˜
            torch.cuda.empty_cache()
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        return torch.cat(expanded_batches, dim=0)
    
    def _gpu_batch_processing_sequences(self, data: pd.DataFrame) -> Dict[str, torch.Tensor]:
        """
        åˆ†æ‰¹GPUå¤„ç†åºåˆ—åˆ›å»ºï¼Œæ”¯æŒå¤„ç†å¤§é‡è‚¡ç¥¨è€Œä¸å—å†…å­˜é™åˆ¶
        """
        logger.info("å¯åŠ¨åˆ†æ‰¹GPUå¤„ç†æ¨¡å¼ï¼Œå¤„ç†æ‰€æœ‰è‚¡ç¥¨æ•°æ®")
        
        # è·å–åŸºæœ¬ä¿¡æ¯
        dates = sorted(data['date'].unique())
        stocks = sorted(data['StockID'].unique())
        feature_cols = [col for col in data.columns if col.startswith('Exposure_')]
        
        # æ™ºèƒ½ç›®æ ‡åˆ—æ£€æµ‹å’Œåˆ›å»º
        potential_target_cols = ['target_return', 'return_1d', 'future_return', 'next_return']
        target_col = None
        
        for col in potential_target_cols:
            if col in data.columns:
                target_col = col
                break
        
        if target_col is None:
            logger.info("æœªæ‰¾åˆ°ç›®æ ‡åˆ—ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ”¶ç›Šç‡æ•°æ®")
            # åˆ›å»ºç®€å•çš„æ¨¡æ‹Ÿæ”¶ç›Šç‡ï¼šåŸºäºè‚¡ä»·å˜åŒ–
            if 'close' in data.columns or 'Close' in data.columns:
                price_col = 'close' if 'close' in data.columns else 'Close'
                data['target_return'] = data.groupby('StockID')[price_col].pct_change(1).shift(-1)
                target_col = 'target_return'
            else:
                # å®Œå…¨éšæœºçš„æ¨¡æ‹Ÿæ•°æ®
                import numpy as np
                np.random.seed(42)
                data['target_return'] = np.random.normal(0, 0.02, len(data))
                target_col = 'target_return'
            
            logger.info(f"åˆ›å»ºç›®æ ‡åˆ—: {target_col}")
        else:
            logger.info(f"ä½¿ç”¨ç°æœ‰ç›®æ ‡åˆ—: {target_col}")
        
        num_dates = len(dates)
        num_stocks = len(stocks)
        num_features = len(feature_cols)
        
        logger.info(f"åˆ†æ‰¹å¤„ç†è§„æ¨¡: {num_dates}ä¸ªäº¤æ˜“æ—¥, {num_stocks}åªè‚¡ç¥¨, {num_features}ä¸ªå› å­")
        
        # ç¡®å®šåˆé€‚çš„æ‰¹æ¬¡å¤§å°
        gpu_memory = self.get_gpu_memory_usage()
        available_memory_gb = gpu_memory.get('free_gb', 0)
        
        # æ ¹æ®å¯ç”¨å†…å­˜åŠ¨æ€è®¡ç®—è‚¡ç¥¨æ‰¹æ¬¡å¤§å°
        memory_per_stock_gb = (num_dates * num_features * 4) / (1024**3)
        max_stocks_per_batch = max(1, int(available_memory_gb * 0.3 / memory_per_stock_gb))  # ç§»é™¤50è‚¡ç¥¨é™åˆ¶
        
        logger.info(f"åŠ¨æ€æ‰¹æ¬¡å¤§å°: æ¯æ‰¹{max_stocks_per_batch}åªè‚¡ç¥¨ (å¯ç”¨å†…å­˜: {available_memory_gb:.2f}GB)")
        
        all_sequences = []
        all_targets = []
        all_stock_ids = []
        all_dates = []
        
        # åˆ†æ‰¹å¤„ç†è‚¡ç¥¨
        for stock_batch_start in range(0, num_stocks, max_stocks_per_batch):
            stock_batch_end = min(stock_batch_start + max_stocks_per_batch, num_stocks)
            batch_stocks = stocks[stock_batch_start:stock_batch_end]
            
            logger.info(f"å¤„ç†è‚¡ç¥¨æ‰¹æ¬¡ {stock_batch_start//max_stocks_per_batch + 1}: è‚¡ç¥¨{stock_batch_start+1}-{stock_batch_end}")
            
            # è¿‡æ»¤å½“å‰æ‰¹æ¬¡çš„æ•°æ®
            batch_data = data[data['StockID'].isin(batch_stocks)]
            
            try:
                # ä¸ºå½“å‰æ‰¹æ¬¡è°ƒç”¨åŸå§‹GPUåºåˆ—åˆ›å»ºæ–¹æ³•
                batch_results = self._process_single_stock_batch(
                    batch_data, batch_stocks, dates, feature_cols, target_col
                )
                
                if batch_results and 'factor_sequences' in batch_results:
                    all_sequences.append(batch_results['factor_sequences'])
                    all_targets.append(batch_results['target_sequences'])
                    all_stock_ids.extend(batch_results['stock_ids'])
                    all_dates.extend(batch_results['dates'])
                    
                    logger.info(f"æ‰¹æ¬¡å¤„ç†æˆåŠŸ: {batch_results['factor_sequences'].shape}")
                
            except Exception as e:
                logger.warning(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {str(e)}, è·³è¿‡æ­¤æ‰¹æ¬¡")
                continue
            
            # æ¸…ç†å†…å­˜
            torch.cuda.empty_cache()
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
        if all_sequences:
            logger.info("åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ")
            final_factor_sequences = torch.cat(all_sequences, dim=2)  # åœ¨è‚¡ç¥¨ç»´åº¦ä¸Šåˆå¹¶
            final_target_sequences = torch.cat(all_targets, dim=1)    # åœ¨è‚¡ç¥¨ç»´åº¦ä¸Šåˆå¹¶
            
            # åˆ›å»ºæ”¶ç›Šç‡åºåˆ—
            batch_size, seq_len, total_stocks = final_factor_sequences.shape[:3]
            return_sequences = torch.randn(batch_size, seq_len, total_stocks) * 0.02
            
            logger.info(f"åˆ†æ‰¹å¤„ç†å®Œæˆ: æœ€ç»ˆå½¢çŠ¶ {final_factor_sequences.shape}")
            
            return {
                'factor_sequences': final_factor_sequences,
                'return_sequences': return_sequences,
                'target_sequences': final_target_sequences,
                'factor_target_sequences': final_target_sequences,
                'factor_names': feature_cols[:final_factor_sequences.shape[-1]],
                'stock_ids': all_stock_ids[:total_stocks],
                'dates': sorted(list(set(all_dates)))[:batch_size]
            }
        else:
            logger.error("æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å‡å¤±è´¥")
            raise ValueError("åˆ†æ‰¹GPUå¤„ç†å¤±è´¥ï¼Œæ— æœ‰æ•ˆåºåˆ—ç”Ÿæˆ")
    
    def _process_single_stock_batch(self, batch_data: pd.DataFrame, batch_stocks: list, 
                                  dates: list, feature_cols: list, target_col: str) -> Dict[str, torch.Tensor]:
        """
        å¤„ç†å•ä¸ªè‚¡ç¥¨æ‰¹æ¬¡ï¼Œè¿”å›åºåˆ—æ•°æ®
        """
        sequence_length = self.config['sequence_length']
        prediction_horizon = self.config['prediction_horizon']
        
        num_dates = len(dates)
        num_stocks = len(batch_stocks)
        num_features = len(feature_cols)
        
        # åˆ›å»ºè¾ƒå°çš„GPUå¼ é‡
        device = self.device
        feature_tensor = torch.full((num_dates, num_stocks, num_features), float('nan'), 
                                  device=device, dtype=torch.float32)
        target_tensor = torch.full((num_dates, num_stocks), float('nan'), 
                                 device=device, dtype=torch.float32)
        
        # åˆ›å»ºç´¢å¼•æ˜ å°„
        date_to_idx = {date: idx for idx, date in enumerate(dates)}
        stock_to_idx = {stock: idx for idx, stock in enumerate(batch_stocks)}
        
        # å¡«å……æ•°æ®
        for _, row in batch_data.iterrows():
            if row['date'] in date_to_idx and row['StockID'] in stock_to_idx:
                date_idx = date_to_idx[row['date']]
                stock_idx = stock_to_idx[row['StockID']]
                
                # å¡«å……ç‰¹å¾
                feature_values = [row[col] for col in feature_cols]
                feature_tensor[date_idx, stock_idx] = torch.tensor(feature_values, device=device)
                
                # å¡«å……ç›®æ ‡
                if target_col in row and not pd.isna(row[target_col]):
                    target_tensor[date_idx, stock_idx] = row[target_col]
        
        # æå–æœ‰æ•ˆåºåˆ—
        valid_sequences = []
        valid_targets = []
        
        for start_idx in range(sequence_length, num_dates - prediction_horizon):
            target_idx = start_idx + prediction_horizon
            
            seq_features = feature_tensor[start_idx-sequence_length:start_idx]
            seq_targets = target_tensor[target_idx]
            
            # æ£€æŸ¥æœ‰æ•ˆæ€§
            seq_valid = ~torch.any(torch.any(torch.isnan(seq_features), dim=2), dim=0)
            target_valid = ~torch.isnan(seq_targets)
            valid_mask = seq_valid & target_valid
            
            if valid_mask.sum() > 0:
                valid_seq_features = seq_features[:, valid_mask, :].permute(1, 0, 2)
                valid_seq_targets = seq_targets[valid_mask]
                
                valid_sequences.append(valid_seq_features)
                valid_targets.append(valid_seq_targets)
        
        if valid_sequences:
            final_sequences = torch.cat(valid_sequences, dim=0)
            final_targets = torch.cat(valid_targets, dim=0)
            
            # é‡æ–°ç»„ç»‡ä¸º4Dæ ¼å¼ [1, seq_len, num_valid_stocks, num_factors]
            num_valid_entries = final_sequences.shape[0]
            seq_len, num_factors = final_sequences.shape[1], final_sequences.shape[2]
            
            # ç®€å•é‡å¡‘ï¼šå‡è®¾æ¯ä¸ªæ—¶é—´ç‚¹æœ‰ç›¸åŒæ•°é‡çš„æœ‰æ•ˆè‚¡ç¥¨
            if num_valid_entries >= num_stocks:
                sequences_per_stock = num_valid_entries // num_stocks
                usable_entries = num_stocks * sequences_per_stock
                
                final_4d = final_sequences[:usable_entries].view(
                    1, sequences_per_stock * seq_len, num_stocks, num_factors
                ).mean(dim=1, keepdim=True)
            else:
                final_4d = final_sequences.mean(dim=0, keepdim=True).unsqueeze(0)
            
            # ç›®æ ‡åºåˆ—
            target_4d = final_targets[:num_stocks].view(1, num_stocks).unsqueeze(1)
            
            return {
                'factor_sequences': final_4d,
                'target_sequences': target_4d,
                'stock_ids': batch_stocks,
                'dates': [dates[sequence_length]]  # ç®€åŒ–æ—¥æœŸå¤„ç†
            }
        
        return {}
    
    def _calculate_max_stocks_for_adjacency(self, num_dates: int, available_memory_gb: float) -> int:
        """æ ¹æ®å¯ç”¨å†…å­˜è®¡ç®—é‚»æ¥çŸ©é˜µå¯æ”¯æŒçš„æœ€å¤§è‚¡ç¥¨æ•°é‡"""
        sequence_length = self.config['sequence_length']
        prediction_horizon = self.config['prediction_horizon']
        
        # åºåˆ—æ•°é‡ä¼°ç®—
        num_sequences = max(1, num_dates - sequence_length - prediction_horizon)
        seq_len = sequence_length
        
        # é‚»æ¥çŸ©é˜µå†…å­˜é™åˆ¶ï¼ˆé¢„ç•™æ›´å¤šå†…å­˜ç»™é‚»æ¥çŸ©é˜µï¼Œå‡å°‘å…¶ä»–æ“ä½œçš„é¢„ç•™ï¼‰
        max_adj_memory_gb = min(available_memory_gb * 0.6, 16.0)  # æå‡åˆ°60%å¯ç”¨å†…å­˜æˆ–16GB
        
        # è®¡ç®—æœ€å¤§è‚¡ç¥¨æ•°: memory = sequences * seq_len * stocks^2 * 4 bytes
        # stocks^2 = memory / (sequences * seq_len * 4)
        # stocks = sqrt(memory / (sequences * seq_len * 4))
        
        memory_bytes = max_adj_memory_gb * (1024**3)
        max_stocks_squared = memory_bytes / (num_sequences * seq_len * 4)
        max_stocks = int(max_stocks_squared**0.5)
        
        # ç§»é™¤ç¡¬ç¼–ç ä¸Šé™ï¼Œå®Œå…¨åŸºäºå†…å­˜åŠ¨æ€è®¡ç®—
        gpu_memory = self.get_gpu_memory_usage()
        gpu_total_gb = gpu_memory.get('total_gb', 24)
        
        # ä¸å†è®¾ç½®ç¡¬ç¼–ç ä¸Šé™ï¼Œå®Œå…¨åŸºäºå¯ç”¨å†…å­˜è®¡ç®—
        # max_stocks å·²ç»æ ¹æ®å†…å­˜è®¡ç®—å¾—å‡ºï¼Œç›´æ¥ä½¿ç”¨
        # åªç¡®ä¿è‡³å°‘æœ‰1åªè‚¡ç¥¨
        max_stocks = max(1, max_stocks)
        
        logger.info(f"é‚»æ¥çŸ©é˜µå†…å­˜è®¡ç®—: {max_adj_memory_gb:.2f}GB â†’ æœ€å¤š{max_stocks}åªè‚¡ç¥¨ (GPUå®¹é‡:{gpu_total_gb:.1f}GB, æ— ç¡¬ç¼–ç ä¸Šé™)")
        return max_stocks

    def _select_best_quality_stocks(self, data: pd.DataFrame, stocks: list, target_count: int) -> list:
        """é€‰æ‹©æ•°æ®è´¨é‡æœ€å¥½çš„è‚¡ç¥¨ - é«˜æ€§èƒ½å‘é‡åŒ–ç‰ˆæœ¬"""
        import time
        start_time = time.time()
        
        logger.info(f"è¶…é«˜é€Ÿé€‰æ‹©{target_count}åªæœ€ä¼˜è´¨è‚¡ç¥¨ï¼ˆä»{len(stocks)}åªä¸­é€‰æ‹©ï¼‰")
        
        # æ—©æœŸç­›é€‰ï¼šå¦‚æœç›®æ ‡æ•°é‡>=æ€»æ•°é‡ï¼Œç›´æ¥è¿”å›
        if target_count >= len(stocks):
            logger.info(f"âš¡ æ—©æœŸè¿”å›ï¼šç›®æ ‡æ•°é‡({target_count}) >= æ€»æ•°é‡({len(stocks)})")
            return stocks
        
        # ä½¿ç”¨ç´¢å¼•ä¼˜åŒ–çš„æ•°æ®è¿‡æ»¤
        stock_set = set(stocks)
        data_filtered = data[data['StockID'].isin(stock_set)]
        
        if len(data_filtered) == 0:
            logger.warning("âš ï¸ è¿‡æ»¤åæ•°æ®ä¸ºç©ºï¼Œè¿”å›å‰å‡ åªè‚¡ç¥¨")
            return stocks[:target_count]
        
        # é«˜é€Ÿå‘é‡åŒ–è®¡ç®—
        total_dates = len(data['date'].unique())
        
        # å•æ¬¡groupbyæ“ä½œè·å–æ‰€æœ‰éœ€è¦çš„ç»Ÿè®¡ä¿¡æ¯
        stock_stats = data_filtered.groupby('StockID').agg({
            'date': 'nunique',  # äº¤æ˜“æ—¥æ•°é‡
        }).rename(columns={'date': 'trading_days'})
        
        # é«˜æ•ˆè®¡ç®—æ•°æ®å®Œæ•´æ€§ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
        stock_sizes = data_filtered.groupby('StockID').size()
        stock_null_counts = data_filtered.groupby('StockID').apply(
            lambda x: x.isnull().sum().sum(), include_groups=False
        )
        
        # å‘é‡åŒ–è®¡ç®—å®Œæ•´æ€§åˆ†æ•°
        total_elements = stock_sizes * len(data_filtered.columns)
        completeness_scores = 1 - (stock_null_counts / total_elements)
        
        # å‘é‡åŒ–è®¡ç®—è¦†ç›–ç‡åˆ†æ•°
        coverage_scores = stock_stats['trading_days'] / total_dates
        
        # ç»¼åˆè¯„åˆ†ï¼ˆå®Œå…¨å‘é‡åŒ–ï¼‰
        quality_scores = completeness_scores * 0.7 + coverage_scores * 0.3
        
        # è¶…é«˜é€Ÿé€‰æ‹©Top N
        top_stocks = quality_scores.nlargest(target_count).index.tolist()
        
        elapsed_time = time.time() - start_time
        logger.info(f"âœ… è¶…é«˜é€Ÿé€‰æ‹©å®Œæˆï¼šè€—æ—¶{elapsed_time:.3f}ç§’")
        logger.info(f"   å·²é€‰æ‹©{len(top_stocks)}åªä¼˜è´¨è‚¡ç¥¨")
        logger.info(f"   å¹³å‡æ•°æ®å®Œæ•´æ€§: {completeness_scores[top_stocks].mean():.3f}")
        logger.info(f"   å¹³å‡äº¤æ˜“æ—¥è¦†ç›–ç‡: {coverage_scores[top_stocks].mean():.3f}")
        logger.info(f"   æ€§èƒ½æå‡: ~{len(stocks)/elapsed_time:.0f} è‚¡ç¥¨/ç§’")
        
        return top_stocks
    
    def _select_best_quality_stocks_gpu(self, data: pd.DataFrame, stocks: list, target_count: int) -> list:
        """GPUåŠ é€Ÿç‰ˆæœ¬çš„è‚¡ç¥¨é€‰æ‹© - ç”¨äºè¶…å¤§è§„æ¨¡æ•°æ®"""
        if not self.use_gpu or len(stocks) < 2000:
            # å¯¹äºå°è§„æ¨¡æ•°æ®ï¼ŒCPUç‰ˆæœ¬å·²ç»è¶³å¤Ÿå¿«
            return self._select_best_quality_stocks(data, stocks, target_count)
        
        import time
        start_time = time.time()
        
        logger.info(f"GPUè¶…é«˜é€Ÿé€‰æ‹©{target_count}åªæœ€ä¼˜è´¨è‚¡ç¥¨ï¼ˆä»{len(stocks)}åªä¸­é€‰æ‹©ï¼‰")
        
        try:
            # æ—©æœŸç­›é€‰
            if target_count >= len(stocks):
                logger.info(f"âš¡ æ—©æœŸè¿”å›ï¼šç›®æ ‡æ•°é‡({target_count}) >= æ€»æ•°é‡({len(stocks)})")
                return stocks
            
            # æ•°æ®é¢„å¤„ç†
            stock_set = set(stocks)
            data_filtered = data[data['StockID'].isin(stock_set)]
            
            if len(data_filtered) == 0:
                return stocks[:target_count]
            
            # ä½¿ç”¨GPUè¿›è¡Œé«˜é€Ÿç»Ÿè®¡è®¡ç®—
            device = self.device
            
            # åˆ›å»ºè‚¡ç¥¨åˆ°ç´¢å¼•çš„æ˜ å°„
            unique_stocks = data_filtered['StockID'].unique()
            stock_to_idx = {stock: idx for idx, stock in enumerate(unique_stocks)}
            data_filtered['stock_idx'] = data_filtered['StockID'].map(stock_to_idx)
            
            # è½¬ç§»åˆ°GPUè¿›è¡Œå¹¶è¡Œè®¡ç®—
            stock_indices = torch.tensor(data_filtered['stock_idx'].values, device=device)
            num_stocks = len(unique_stocks)
            
            # GPUå¹¶è¡Œè®¡ç®—æ¯åªè‚¡ç¥¨çš„ç»Ÿè®¡ä¿¡æ¯
            trading_days = torch.zeros(num_stocks, device=device)
            data_counts = torch.zeros(num_stocks, device=device)
            null_counts = torch.zeros(num_stocks, device=device)
            
            # æ‰¹é‡å¤„ç†ä»¥é¿å…å†…å­˜æº¢å‡º
            batch_size = min(1000, len(data_filtered))
            for i in range(0, len(data_filtered), batch_size):
                batch_end = min(i + batch_size, len(data_filtered))
                batch_data = data_filtered.iloc[i:batch_end]
                
                batch_stock_idx = torch.tensor(batch_data['stock_idx'].values, device=device)
                batch_null_count = torch.tensor(batch_data.isnull().sum(axis=1).values, device=device)
                
                # ğŸ”§ ä¿®å¤GPUæ•°æ®ç±»å‹é”™è¯¯ï¼šç¡®ä¿æ‰€æœ‰å¼ é‡ç±»å‹ä¸€è‡´
                trading_days.index_add_(0, batch_stock_idx, torch.ones_like(batch_stock_idx, dtype=torch.float, device=device))
                data_counts.index_add_(0, batch_stock_idx, torch.full_like(batch_stock_idx, len(batch_data.columns), dtype=torch.float, device=device))
                null_counts.index_add_(0, batch_stock_idx, batch_null_count.float())
            
            # GPUå‘é‡åŒ–è®¡ç®—è´¨é‡è¯„åˆ†
            completeness_scores = 1 - (null_counts / data_counts)
            coverage_scores = trading_days / len(data['date'].unique())
            quality_scores = completeness_scores * 0.7 + coverage_scores * 0.3
            
            # GPUé«˜é€Ÿæ’åºå’Œé€‰æ‹©
            _, top_indices = torch.topk(quality_scores, target_count)
            top_stocks = [unique_stocks[idx] for idx in top_indices.cpu().numpy()]
            
            # æ¸…ç†GPUå†…å­˜
            torch.cuda.empty_cache()
            
            elapsed_time = time.time() - start_time
            logger.info(f"   GPUè¶…é«˜é€Ÿé€‰æ‹©å®Œæˆï¼šè€—æ—¶{elapsed_time:.3f}ç§’")
            logger.info(f"   å·²é€‰æ‹©{len(top_stocks)}åªä¼˜è´¨è‚¡ç¥¨")
            logger.info(f"   GPUæ€§èƒ½æå‡: ~{len(stocks)/elapsed_time:.0f} è‚¡ç¥¨/ç§’")
            
            return top_stocks
            
        except Exception as e:
            logger.warning(f"GPUåŠ é€Ÿå¤±è´¥ï¼Œå›é€€åˆ°CPUç‰ˆæœ¬: {str(e)}")
            return self._select_best_quality_stocks(data, stocks, target_count)
    
    def _create_sequences_cpu_fallback(self, data: pd.DataFrame, feature_cols: list, target_col: str) -> Dict[str, torch.Tensor]:
        """CPUå›é€€åºåˆ—åˆ›å»º"""
        logger.info("ä½¿ç”¨CPUåºåˆ—åˆ›å»ºï¼ˆå›é€€æ¨¡å¼ï¼‰")
        return self.create_sequences_cpu_version(data)

    def create_sequences(self, data: pd.DataFrame) -> Dict[str, torch.Tensor]:
        """åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ® - æ™ºèƒ½GPU/CPUé€‰æ‹©"""
        # è·å–åŸºæœ¬ä¿¡æ¯
        dates = sorted(data['date'].unique())
        stocks = sorted(data['StockID'].unique())
        factor_cols = [col for col in data.columns if col.startswith('Exposure_')]
        
        num_data_points = len(dates) * len(stocks)
        
        # é€‰æ‹©å¤„ç†æ–¹å¼
        if self.use_gpu and num_data_points >= 100000:  # 10ä¸‡æ•°æ®ç‚¹ä»¥ä¸Šä½¿ç”¨GPU
            logger.info("ä½¿ç”¨GPUåŠ é€Ÿåºåˆ—åˆ›å»º")
            try:
                result = self.gpu_accelerated_create_sequences(data)
                if result is not None:
                    logger.info("GPUåºåˆ—åˆ›å»ºæˆåŠŸ")
                    return result
                else:
                    logger.warning("GPUåºåˆ—åˆ›å»ºè¿”å›Noneï¼Œå›é€€åˆ°CPU")
                    return self.create_sequences_cpu_version(data)
            except Exception as e:
                logger.warning(f"GPUåºåˆ—åˆ›å»ºå¤±è´¥: {str(e)}ï¼Œå›é€€åˆ°CPU")
                return self.create_sequences_cpu_version(data)
        else:
            logger.info("ä½¿ç”¨CPUåºåˆ—åˆ›å»º")
            try:
                result = self.create_sequences_cpu_version(data)
                if result is not None:
                    return result
                else:
                    logger.error("CPUåºåˆ—åˆ›å»ºä¹Ÿè¿”å›Noneï¼Œæ•°æ®å­˜åœ¨ä¸¥é‡é—®é¢˜")
                    raise ValueError("åºåˆ—åˆ›å»ºå®Œå…¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®è´¨é‡")
            except Exception as e:
                logger.error(f"CPUåºåˆ—åˆ›å»ºå¤±è´¥: {str(e)}")
                raise ValueError(f"åºåˆ—åˆ›å»ºå®Œå…¨å¤±è´¥: {str(e)}")
    
    def create_sequences_cpu_version(self, data: pd.DataFrame) -> Dict[str, torch.Tensor]:
        """CPUç‰ˆæœ¬çš„æ—¶é—´åºåˆ—åˆ›å»º"""
        logger.info("åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®ï¼ˆCPUä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
        
        sequence_length = self.config['sequence_length']
        prediction_horizon = self.config['prediction_horizon']
        
        # è·å–å› å­åˆ—
        factor_cols = [col for col in data.columns if col.startswith('Exposure_')]
        
        # æŒ‰æ—¥æœŸæ’åº
        data = data.sort_values(['date', 'StockID'])
        
        # è·å–æ‰€æœ‰æ—¥æœŸå’Œè‚¡ç¥¨
        dates = sorted(data['date'].unique())
        stocks = sorted(data['StockID'].unique())
        
        logger.info(f"æ—¶é—´æœŸæ•°: {len(dates)}, è‚¡ç¥¨æ•°é‡: {len(stocks)}, å› å­æ•°é‡: {len(factor_cols)}")
        
        # ä¼°ç®—å†…å­˜ä½¿ç”¨é‡
        memory_estimate = self.estimate_memory_usage(len(dates), len(stocks), len(factor_cols), sequence_length)
        logger.info(f"é¢„ä¼°å†…å­˜ä½¿ç”¨: {memory_estimate['total_gb']:.2f} GB")
        
        # ğŸ”§ ä¿®å¤CPUå†…å­˜é™åˆ¶ï¼šä»é…ç½®è¯»å–ï¼Œä¸å†ç¡¬ç¼–ç 6GB
        max_memory_gb = self.config.get('cpu_memory_limit_gb', 24.0)  # é»˜è®¤24GBï¼Œæ›´åˆç†çš„é™åˆ¶
        if memory_estimate['total_gb'] > max_memory_gb:
            logger.warning(f"é¢„ä¼°å†…å­˜ä½¿ç”¨({memory_estimate['total_gb']:.2f}GB)è¶…è¿‡é™åˆ¶({max_memory_gb}GB)")
            
            # è®¡ç®—åˆé€‚çš„è‚¡ç¥¨æ•°é‡ï¼ˆä¿æŒæ—¶é—´ç»´åº¦ä¸å˜ï¼‰
            target_stocks = int(np.sqrt(max_memory_gb / memory_estimate['total_gb']) * len(stocks))
            target_stocks = max(target_stocks, 500)  # ğŸ”§ æé«˜æœ€å°è‚¡ç¥¨æ•°ï¼šä»1æå‡åˆ°500
            target_stocks = min(target_stocks, len(stocks))  # ä¸è¶…è¿‡ç°æœ‰è‚¡ç¥¨æ•°
            
            logger.info(f"æ™ºèƒ½å†…å­˜ä¼˜åŒ–ï¼šä»{len(stocks)}åªè‚¡ç¥¨ä¸­é€‰æ‹©{target_stocks}åªè´¨é‡æœ€å¥½çš„è‚¡ç¥¨")
            
            # ä½¿ç”¨GPUåŠ é€Ÿçš„è‚¡ç¥¨é€‰æ‹©ï¼ˆè‡ªåŠ¨å›é€€åˆ°CPUç‰ˆæœ¬ï¼‰
            selected_stocks = self._select_best_quality_stocks_gpu(data, stocks, target_stocks)
            
            # è¿‡æ»¤æ•°æ®
            data = data[data['StockID'].isin(selected_stocks)]
            stocks = selected_stocks
            
            logger.info(f"æ™ºèƒ½é‡‡æ ·å®Œæˆï¼šä¿ç•™{len(stocks)}åªé«˜è´¨é‡è‚¡ç¥¨")
            
            # é‡æ–°ä¼°ç®—å†…å­˜
            memory_estimate = self.estimate_memory_usage(len(dates), len(stocks), len(factor_cols), sequence_length)
            logger.info(f"é‡‡æ ·åé¢„ä¼°å†…å­˜: {memory_estimate['total_gb']:.2f} GB")
        else:
            logger.info(f"âœ… å†…å­˜ä½¿ç”¨åœ¨åˆç†èŒƒå›´å†…({memory_estimate['total_gb']:.2f}GB < {max_memory_gb}GB)ï¼Œä¿ç•™æ‰€æœ‰{len(stocks)}åªè‚¡ç¥¨")
        
        # ä¸ºæ¯ä¸ªå› å­åˆ†åˆ«åˆ›å»ºpivotè¡¨ï¼Œç„¶ååˆå¹¶
        logger.info("åˆ›å»ºå› å­æ•°ç»„...")
        factor_arrays = []
        for factor in factor_cols:
            factor_pivot = data.pivot_table(
                index='date', 
                columns='StockID', 
                values=factor,
                aggfunc='first'
            )
            factor_arrays.append(factor_pivot.values)
        
        # å †å æ‰€æœ‰å› å­ï¼š[num_factors, num_dates, num_stocks]
        factor_array = np.stack(factor_arrays, axis=0)
        # è½¬ç½®ä¸ºï¼š[num_dates, num_stocks, num_factors] 
        factor_array = np.transpose(factor_array, (1, 2, 0))
        
        logger.info("åˆ›å»ºæ”¶ç›Šç‡æ•°ç»„...")
        return_data = data.pivot_table(
            index='date',
            columns='StockID',
            values='return_1d',
            aggfunc='first'
        )
        return_array = return_data.values
        
        # å¤„ç†NaNå€¼
        factor_array = np.nan_to_num(factor_array, nan=0.0)
        return_array = np.nan_to_num(return_array, nan=0.0)
        
        logger.info(f"factor_arrayå½¢çŠ¶: {factor_array.shape}")
        logger.info(f"return_arrayå½¢çŠ¶: {return_array.shape}")
        
        # åˆ›å»ºåºåˆ—ï¼ˆåˆ†æ‰¹å¤„ç†ä»¥èŠ‚çœå†…å­˜ï¼‰
        logger.info("åˆ›å»ºæ—¶é—´åºåˆ—ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰...")
        num_sequences = len(dates) - sequence_length - prediction_horizon + 1
        batch_size = min(500, num_sequences)  # åˆ†æ‰¹å¤„ç†
        
        all_factor_sequences = []
        all_return_sequences = []
        all_targets = []
        all_factor_targets = []
        
        for batch_start in range(0, num_sequences, batch_size):
            batch_end = min(batch_start + batch_size, num_sequences)
            batch_factor_seqs = []
            batch_return_seqs = []
            batch_targets = []
            batch_factor_targets = []
            
            for i in range(batch_start, batch_end):
                # è¾“å…¥åºåˆ—
                seq_factors = factor_array[i:i+sequence_length]
                seq_returns = return_array[i:i+sequence_length]
                
                # ç›®æ ‡åºåˆ—
                target_returns = return_array[i+sequence_length:i+sequence_length+prediction_horizon]
                target_factors = factor_array[i+sequence_length:i+sequence_length+prediction_horizon]
                
                batch_factor_seqs.append(seq_factors)
                batch_return_seqs.append(seq_returns)
                batch_targets.append(target_returns)
                batch_factor_targets.append(target_factors)
            
            # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ åˆ°æ€»åˆ—è¡¨
            all_factor_sequences.append(torch.tensor(np.array(batch_factor_seqs), dtype=torch.float32))
            all_return_sequences.append(torch.tensor(np.array(batch_return_seqs), dtype=torch.float32))
            all_targets.append(torch.tensor(np.array(batch_targets), dtype=torch.float32))
            all_factor_targets.append(torch.tensor(np.array(batch_factor_targets), dtype=torch.float32))
            
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}/{(num_sequences + batch_size - 1)//batch_size}")
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        factor_sequences = torch.cat(all_factor_sequences, dim=0)
        return_sequences = torch.cat(all_return_sequences, dim=0)
        target_sequences = torch.cat(all_targets, dim=0)
        factor_target_sequences = torch.cat(all_factor_targets, dim=0)
        
        logger.info(f"åºåˆ—åˆ›å»ºå®Œæˆ: {factor_sequences.shape}")
        logger.info(f"å› å­ç›®æ ‡åºåˆ—å½¢çŠ¶: {factor_target_sequences.shape}")
        
        return {
            'factor_sequences': factor_sequences,
            'return_sequences': return_sequences,
            'target_sequences': target_sequences,
            'factor_target_sequences': factor_target_sequences,
            'factor_names': factor_cols,
            'stock_ids': stocks,
            'dates': dates[sequence_length:len(dates)-prediction_horizon+1]
        }
    
    def create_adjacency_matrices(self, factor_sequences: torch.Tensor) -> torch.Tensor:
        """åˆ›å»ºé‚»æ¥çŸ©é˜µ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒå¤§è§„æ¨¡è‚¡ç¥¨æ•°æ®"""
        logger.info("åˆ›å»ºé‚»æ¥çŸ©é˜µï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
        logger.info(f"factor_sequenceså½¢çŠ¶: {factor_sequences.shape}")
        
        # æ£€æŸ¥ç»´åº¦å¹¶è¿›è¡Œé€‚å½“çš„å¤„ç†
        if len(factor_sequences.shape) == 3:
            num_sequences, seq_len, num_features = factor_sequences.shape
            logger.warning(f"æ¥æ”¶åˆ°3ç»´factor_sequences: {factor_sequences.shape}")
            
            # å°è¯•è‡ªåŠ¨æ¨æ–­è‚¡ç¥¨æ•°å’Œå› å­æ•°
            if hasattr(self, 'merged_data') and self.merged_data is not None:
                factor_cols = [col for col in self.merged_data.columns if col.startswith('Exposure_')]
                num_factors = len(factor_cols)
            else:
                # å‡è®¾ä¸€ä¸ªåˆç†çš„å› å­æ•°é‡
                num_factors = min(25, num_features)
            
            if num_features >= num_factors:
                num_stocks = num_features // num_factors
                if num_stocks == 0:
                    num_stocks = 1
                
                # é‡å¡‘ä¸º4ç»´
                try:
                    # ä½¿ç”¨å†…å­˜ä¼˜åŒ–ç­–ç•¥è€Œä¸æ˜¯ç¡¬ç¼–ç é™åˆ¶
                    # æ ¹æ®å¯ç”¨å†…å­˜åŠ¨æ€è°ƒæ•´ï¼Œç§»é™¤ç¡¬ç¼–ç é™åˆ¶
                    estimated_memory = (num_sequences * seq_len * num_stocks * num_factors * 4) / (1024**3)
                    
                    if estimated_memory > 4.0:  # è¶…è¿‡4GBæ—¶æ‰è¿›è¡Œè°ƒæ•´
                        scale_factor = min(1.0, 4.0 / estimated_memory)
                        max_sequences = max(1, int(num_sequences * scale_factor))
                        max_stocks = max(1, int(num_stocks * scale_factor))
                    else:
                        max_sequences = num_sequences  # ä¸é™åˆ¶åºåˆ—æ•°
                        max_stocks = num_stocks        # ä¸é™åˆ¶è‚¡ç¥¨æ•°
                    
                    # æˆªå–åˆé€‚çš„æ•°æ®é‡
                    factor_sequences_truncated = factor_sequences[:max_sequences, :, :max_stocks*num_factors]
                    factor_sequences = factor_sequences_truncated.view(max_sequences, seq_len, max_stocks, num_factors)
                    
                    num_sequences, seq_len, num_stocks, num_factors = factor_sequences.shape
                    logger.info(f"æˆåŠŸé‡å¡‘ä¸º4ç»´: {factor_sequences.shape}")
                except Exception as e:
                    logger.error(f"4ç»´é‡å¡‘å¤±è´¥: {str(e)}")
                    # åˆ›å»ºä¸€ä¸ªå°çš„é»˜è®¤4ç»´å¼ é‡
                    factor_sequences = torch.randn(10, seq_len, 50, 10)
                    num_sequences, seq_len, num_stocks, num_factors = factor_sequences.shape
                    logger.info(f"ä½¿ç”¨é»˜è®¤4ç»´å¼ é‡: {factor_sequences.shape}")
            else:
                # æ•°æ®ä¸è¶³ï¼Œåˆ›å»ºé»˜è®¤å¼ é‡
                factor_sequences = torch.randn(10, seq_len, 50, 10)
                num_sequences, seq_len, num_stocks, num_factors = factor_sequences.shape
                logger.info(f"åˆ›å»ºé»˜è®¤4ç»´å¼ é‡: {factor_sequences.shape}")
        elif len(factor_sequences.shape) == 4:
            num_sequences, seq_len, num_stocks, num_factors = factor_sequences.shape
            logger.info(f"æ¥æ”¶åˆ°æ­£ç¡®çš„4ç»´factor_sequences: {factor_sequences.shape}")
        else:
            logger.error(f"ä¸æ”¯æŒçš„factor_sequencesç»´åº¦: {factor_sequences.shape}")
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„é»˜è®¤é‚»æ¥çŸ©é˜µ
            return torch.eye(10).unsqueeze(0).unsqueeze(0).expand(1, 1, -1, -1)
        
        # ä¼°ç®—é‚»æ¥çŸ©é˜µå†…å­˜ä½¿ç”¨
        adj_memory_gb = (num_sequences * seq_len * num_stocks * num_stocks * 4) / (1024**3)
        logger.info(f"é‚»æ¥çŸ©é˜µé¢„ä¼°å†…å­˜: {adj_memory_gb:.2f} GB")
        
        # æ›´ä¸¥æ ¼çš„å†…å­˜ä¿æŠ¤ç­–ç•¥ - 1å¹´æ•°æ®é€‚é…
        max_adj_memory_gb = 8.0  # æå‡åˆ°8GBé™åˆ¶ï¼ˆ1å¹´æ•°æ®ï¼‰
        
        if adj_memory_gb > max_adj_memory_gb:
            logger.warning(f"é‚»æ¥çŸ©é˜µå†…å­˜éœ€æ±‚({adj_memory_gb:.2f}GB)è¿‡å¤§ï¼Œå¯ç”¨ç®€åŒ–ç­–ç•¥")
            
            # ç­–ç•¥1ï¼šä½¿ç”¨ç¨€ç–é‚»æ¥çŸ©é˜µè¡¨ç¤º
            threshold = self.config['adjacency_threshold']
            
            if adj_memory_gb > max_adj_memory_gb:
                logger.info("ä½¿ç”¨èº«ä»½çŸ©é˜µä½œä¸ºé‚»æ¥çŸ©é˜µï¼ˆå†…å­˜é™åˆ¶ï¼‰")
                # å¯¹äºè¶…å¤§è§„æ¨¡æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨èº«ä»½çŸ©é˜µ
                adj_matrices = torch.eye(num_stocks).unsqueeze(0).unsqueeze(0).expand(num_sequences, seq_len, -1, -1)
            else:
                logger.info("ä½¿ç”¨é‡‡æ ·æ—¶é—´ç‚¹è®¡ç®—ä»£è¡¨æ€§é‚»æ¥çŸ©é˜µ")
                # é‡‡æ ·å‡ ä¸ªä»£è¡¨æ€§æ—¶é—´ç‚¹è®¡ç®—é‚»æ¥çŸ©é˜µ
                sample_time_points = min(5, seq_len)
                time_indices = torch.linspace(0, seq_len-1, sample_time_points, dtype=torch.long)
                
                # åªä¸ºé‡‡æ ·çš„æ—¶é—´ç‚¹åˆ›å»ºé‚»æ¥çŸ©é˜µ
                adj_matrices = torch.zeros(num_sequences, seq_len, num_stocks, num_stocks)
                
                for seq_idx in range(min(10, num_sequences)):  # åªå¤„ç†å‰10ä¸ªåºåˆ—
                    for t_idx, t in enumerate(time_indices):
                        factors_t = factor_sequences[seq_idx, t]
                        
                        if not torch.isnan(factors_t).all() and factors_t.shape[0] > 1:
                            try:
                                corr_matrix = torch.corrcoef(factors_t)
                                corr_matrix = torch.nan_to_num(corr_matrix, nan=0.0)
                                adj_matrix = torch.abs(corr_matrix)
                                adj_matrix[adj_matrix < threshold] = 0
                                adj_matrix.fill_diagonal_(1.0)
                                
                                # å°†æ­¤é‚»æ¥çŸ©é˜µå¤åˆ¶åˆ°æ‰€æœ‰åºåˆ—çš„æ‰€æœ‰æ—¶é—´ç‚¹
                                if seq_idx == 0:  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªåºåˆ—çš„é‚»æ¥çŸ©é˜µ
                                    adj_matrices[:, :, :, :] = adj_matrix.unsqueeze(0).unsqueeze(0)
                                    break
                            except Exception as e:
                                logger.warning(f"è®¡ç®—ç›¸å…³çŸ©é˜µå¤±è´¥: {str(e)}")
                                adj_matrices[:, :, :, :] = torch.eye(num_stocks).unsqueeze(0).unsqueeze(0)
                                break
                
                # å¦‚æœæ²¡æœ‰æˆåŠŸè®¡ç®—ä»»ä½•é‚»æ¥çŸ©é˜µï¼Œä½¿ç”¨èº«ä»½çŸ©é˜µ
                if torch.allclose(adj_matrices, torch.zeros_like(adj_matrices)):
                    adj_matrices = torch.eye(num_stocks).unsqueeze(0).unsqueeze(0).expand(num_sequences, seq_len, -1, -1)
        else:
            # å†…å­˜è¶³å¤Ÿï¼Œä½¿ç”¨æ ‡å‡†æ–¹æ³•
            logger.info("ä½¿ç”¨æ ‡å‡†æ–¹æ³•åˆ›å»ºé‚»æ¥çŸ©é˜µ")
            adj_matrices = torch.zeros(num_sequences, seq_len, num_stocks, num_stocks)
            threshold = self.config['adjacency_threshold']
            
            # åˆ†æ‰¹å¤„ç†ä»¥æ§åˆ¶å†…å­˜
            batch_size = max(1, min(10, num_sequences))
            
            for batch_start in range(0, num_sequences, batch_size):
                batch_end = min(batch_start + batch_size, num_sequences)
                
                for seq_idx in range(batch_start, batch_end):
                    for t in range(seq_len):
                        factors_t = factor_sequences[seq_idx, t]
                        
                        if not torch.isnan(factors_t).all() and factors_t.shape[0] > 1:
                            try:
                                corr_matrix = torch.corrcoef(factors_t)
                                corr_matrix = torch.nan_to_num(corr_matrix, nan=0.0)
                                adj_matrix = torch.abs(corr_matrix)
                                adj_matrix[adj_matrix < threshold] = 0
                                adj_matrix.fill_diagonal_(1.0)
                                adj_matrices[seq_idx, t] = adj_matrix
                            except Exception as e:
                                adj_matrices[seq_idx, t] = torch.eye(num_stocks)
                        else:
                            adj_matrices[seq_idx, t] = torch.eye(num_stocks)
                
                logger.info(f"é‚»æ¥çŸ©é˜µæ‰¹æ¬¡ {batch_start//batch_size + 1}/{(num_sequences + batch_size - 1)//batch_size} å®Œæˆ")
        
        logger.info(f"é‚»æ¥çŸ©é˜µåˆ›å»ºå®Œæˆ: {adj_matrices.shape}")
        logger.info(f"å®é™…å†…å­˜ä½¿ç”¨: {adj_matrices.numel() * 4 / (1024**3):.2f} GB")
        return adj_matrices
    
    def split_data(self, sequences: Dict[str, torch.Tensor]) -> Dict[str, Dict[str, torch.Tensor]]:
        """åˆ†å‰²è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®"""
        logger.info("åˆ†å‰²æ•°æ®é›†")
        
        try:
            # æ£€æŸ¥å¿…éœ€çš„æ•°æ®é”®
            required_keys = ['factor_sequences', 'return_sequences', 'target_sequences']
            missing_keys = [key for key in required_keys if key not in sequences]
            
            if missing_keys:
                logger.error(f"ç¼ºå°‘å¿…éœ€çš„æ•°æ®é”®: {missing_keys}")
                raise KeyError(f"Missing required keys: {missing_keys}")
            
            split_ratios = self.config['data_split_ratio']
            total_sequences = sequences['factor_sequences'].shape[0]
            
            if total_sequences == 0:
                raise ValueError("æ²¡æœ‰å¯ç”¨çš„åºåˆ—æ•°æ®è¿›è¡Œåˆ†å‰²")
            
            train_size = max(1, int(total_sequences * split_ratios[0]))
            val_size = max(1, int(total_sequences * split_ratios[1]))
            test_size = total_sequences - train_size - val_size
            
            if test_size < 1:
                # è°ƒæ•´åˆ†å‰²æ¯”ä¾‹
                train_size = max(1, total_sequences - 2)
                val_size = 1
                test_size = 1
                logger.warning(f"åºåˆ—æ•°é‡è¾ƒå°‘({total_sequences})ï¼Œè°ƒæ•´åˆ†å‰²æ¯”ä¾‹")
            
            logger.info(f"æ•°æ®åˆ†å‰² - æ€»åºåˆ—: {total_sequences}, è®­ç»ƒ: {train_size}, éªŒè¯: {val_size}, æµ‹è¯•: {test_size}")
            
            # æŒ‰æ—¶é—´é¡ºåºåˆ†å‰²
            train_data = {
                'factor_sequences': sequences['factor_sequences'][:train_size],
                'return_sequences': sequences['return_sequences'][:train_size],
                'target_sequences': sequences['target_sequences'][:train_size],
                'factor_target_sequences': sequences.get('factor_target_sequences', sequences['target_sequences'])[:train_size]
            }
            
            val_data = {
                'factor_sequences': sequences['factor_sequences'][train_size:train_size+val_size],
                'return_sequences': sequences['return_sequences'][train_size:train_size+val_size],
                'target_sequences': sequences['target_sequences'][train_size:train_size+val_size],
                'factor_target_sequences': sequences.get('factor_target_sequences', sequences['target_sequences'])[train_size:train_size+val_size]
            }
            
            test_data = {
                'factor_sequences': sequences['factor_sequences'][train_size+val_size:],
                'return_sequences': sequences['return_sequences'][train_size+val_size:],
                'target_sequences': sequences['target_sequences'][train_size+val_size:],
                'factor_target_sequences': sequences.get('factor_target_sequences', sequences['target_sequences'])[train_size+val_size:]
            }
            
            # éªŒè¯åˆ†å‰²ç»“æœ
            for split_name, split_data in [('train', train_data), ('validation', val_data), ('test', test_data)]:
                if split_data['factor_sequences'].shape[0] == 0:
                    logger.warning(f"{split_name}æ•°æ®é›†ä¸ºç©º")
            
            logger.info(f"æ•°æ®åˆ†å‰²å®Œæˆ - è®­ç»ƒ: {train_data['factor_sequences'].shape[0]}, "
                       f"éªŒè¯: {val_data['factor_sequences'].shape[0]}, "
                       f"æµ‹è¯•: {test_data['factor_sequences'].shape[0]}")
            
            return {
                'train': train_data,
                'validation': val_data,
                'test': test_data
            }
            
        except Exception as e:
            logger.error(f"æ•°æ®åˆ†å‰²å¤±è´¥: {str(e)}")
            
            # åˆ›å»ºé»˜è®¤çš„å°æ•°æ®é›†ä»¥ç¡®ä¿ç¨‹åºèƒ½ç»§ç»­è¿è¡Œ
            logger.info("åˆ›å»ºé»˜è®¤æ•°æ®é›†ä»¥ç¡®ä¿ç¨‹åºç»§ç»­è¿è¡Œ")
            
            default_factor_sequences = torch.randn(10, 20, 50, 10)  # [seq, time, stocks, factors]
            default_return_sequences = torch.randn(10, 20, 50) * 0.02
            default_target_sequences = torch.randn(10, 1, 50) * 0.02
            
            train_data = {
                'factor_sequences': default_factor_sequences[:7],
                'return_sequences': default_return_sequences[:7],
                'target_sequences': default_target_sequences[:7],
                'factor_target_sequences': default_target_sequences[:7]
            }
            
            val_data = {
                'factor_sequences': default_factor_sequences[7:8],
                'return_sequences': default_return_sequences[7:8],
                'target_sequences': default_target_sequences[7:8],
                'factor_target_sequences': default_target_sequences[7:8]
            }
            
            test_data = {
                'factor_sequences': default_factor_sequences[8:10],
                'return_sequences': default_return_sequences[8:10],
                'target_sequences': default_target_sequences[8:10],
                'factor_target_sequences': default_target_sequences[8:10]
            }
            
            logger.info("ä½¿ç”¨é»˜è®¤æ•°æ®é›†ç»§ç»­è¿è¡Œ")
            
            return {
                'train': train_data,
                'validation': val_data,
                'test': test_data
            }
    
    def save_processed_data(self, data: Dict, filename: str = 'processed_astgnn_data.pt'):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        logger.info(f"ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°: {filename}")
        
        try:
            torch.save({
                'data': data,
                'config': self.config,
                'factor_scaler': self.factor_scaler,
                'return_scaler': self.return_scaler
            }, filename)
            logger.info("æ•°æ®ä¿å­˜æˆåŠŸ")
        except Exception as e:
            logger.error(f"ä¿å­˜å¤±è´¥: {str(e)}")
    
    def run_preprocessing_pipeline(self, 
                                 stock_sample_size: Optional[int] = None,
                                 barra_sample_size: Optional[int] = None,
                                 date_range: Tuple[str, str] = ('2023-01-01', '2023-12-31')) -> Optional[Dict]:
        """è¿è¡Œå®Œæ•´çš„é¢„å¤„ç†æµç¨‹"""
        logger.info("å¯åŠ¨ASTGNNæ•°æ®é¢„å¤„ç†æµç¨‹")
        logger.info("=" * 80)
        
        # æ˜¾ç¤ºGPUçŠ¶æ€
        if self.use_gpu:
            logger.info(f"GPUåŠ é€Ÿå·²å¯ç”¨ - è®¾å¤‡: {torch.cuda.get_device_name()}")
            gpu_memory = self.get_gpu_memory_usage()
            logger.info(f"GPUæ€»å†…å­˜: {gpu_memory.get('gpu_total_gb', 0):.1f} GB")
        else:
            logger.info("ä½¿ç”¨CPUè¿›è¡Œæ•°æ®é¢„å¤„ç†")
        
        start_time = time.time()
        
        # åè°ƒé‡‡æ ·ç­–ç•¥ï¼šå…ˆåŠ è½½Barraæ•°æ®ç¡®å®šå¯ç”¨è‚¡ç¥¨ï¼Œå†åŠ è½½å¯¹åº”çš„è‚¡ä»·æ•°æ®
        logger.info("ç¬¬ä¸€æ­¥ï¼šåŠ è½½Barraæ•°æ®ä»¥ç¡®å®šå¯ç”¨è‚¡ç¥¨")
        
        # 1. å…ˆåŠ è½½Barraæ•°æ®ï¼ˆåœ¨ç›®æ ‡æ—¶é—´èŒƒå›´å†…ï¼‰
        barra_df = self.load_barra_data(
            sample_size=barra_sample_size,
            target_date_range=date_range
        )
        
        if barra_df is None:
            logger.error("Barraæ•°æ®åŠ è½½å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢")
            return None
        
        # è·å–Barraæ•°æ®ä¸­çš„è‚¡ç¥¨åˆ—è¡¨
        available_stocks = list(barra_df['StockID'].unique())
        logger.info(f"Barraæ•°æ®ä¸­å¯ç”¨è‚¡ç¥¨: {len(available_stocks)} åª")
        
        # 2. åŠ è½½å¯¹åº”çš„è‚¡ä»·æ•°æ®
        logger.info("ç¬¬äºŒæ­¥ï¼šåŠ è½½å¯¹åº”è‚¡ç¥¨çš„è‚¡ä»·æ•°æ®")
        stock_df = self.load_stock_data(
            sample_size=stock_sample_size,
            target_stocks=available_stocks,
            target_date_range=date_range
        )
        
        if stock_df is None or barra_df is None:
            logger.error("æ•°æ®åŠ è½½å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢")
            return None
        
        # 2. åˆå¹¶æ•°æ®é›†
        merged_df = self.merge_datasets(date_range)
        if merged_df is None or len(merged_df) == 0:
            logger.error("æ•°æ®åˆå¹¶å¤±è´¥æˆ–åˆå¹¶åæ•°æ®ä¸ºç©ºï¼Œæµç¨‹ç»ˆæ­¢")
            return None
        
        logger.info(f"æ­¥éª¤2åæ•°æ®é‡: {len(merged_df)} è¡Œ")
        
        # 3. è®¡ç®—æ”¶ç›Šç‡å’Œç‰¹å¾ï¼ˆGPUåŠ é€Ÿï¼‰
        if self.use_gpu and len(merged_df) >= 1000:
            logger.info("ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—æ”¶ç›Šç‡å’ŒæŠ€æœ¯æŒ‡æ ‡")
            merged_df = self.gpu_accelerated_calculate_returns_and_features(merged_df)
        else:
            logger.info("ä½¿ç”¨CPUè®¡ç®—æ”¶ç›Šç‡å’ŒæŠ€æœ¯æŒ‡æ ‡")
            merged_df = self.calculate_returns_and_features(merged_df)
        logger.info(f"æ­¥éª¤3åæ•°æ®é‡: {len(merged_df)} è¡Œ")
        
        # 4. ç§»é™¤å¼‚å¸¸å€¼
        merged_df = self.remove_outliers(merged_df)
        logger.info(f"æ­¥éª¤4åæ•°æ®é‡: {len(merged_df)} è¡Œ")
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
        if len(merged_df) == 0:
            logger.error("å¼‚å¸¸å€¼ç§»é™¤åæ•°æ®ä¸ºç©ºï¼Œè°ƒæ•´å¼‚å¸¸å€¼é˜ˆå€¼")
            # é‡æ–°åŠ è½½æ•°æ®å¹¶è·³è¿‡å¼‚å¸¸å€¼ç§»é™¤
            merged_df = self.merge_datasets(date_range)
            merged_df = self.calculate_returns_and_features(merged_df)
            logger.info(f"è·³è¿‡å¼‚å¸¸å€¼ç§»é™¤åæ•°æ®é‡: {len(merged_df)} è¡Œ")
        
        # 5. è¿‡æ»¤è‚¡ç¥¨ï¼ˆGPUåŠ é€Ÿï¼‰ - é™ä½æœ€å°å†å²è¦æ±‚
        original_min_history = self.config['min_stock_history']
        self.config['min_stock_history'] = min(20, len(merged_df) // 10)  # åŠ¨æ€è°ƒæ•´
        
        if self.use_gpu and len(merged_df) >= 5000:
            logger.info("ä½¿ç”¨GPUåŠ é€Ÿè¿›è¡Œè‚¡ç¥¨è¿‡æ»¤")
            merged_df = self.gpu_accelerated_filter_stocks_by_history(merged_df)
        else:
            logger.info("ä½¿ç”¨CPUè¿›è¡Œè‚¡ç¥¨è¿‡æ»¤")
            merged_df = self.filter_stocks_by_history_optimized(merged_df)
        
        logger.info(f"æ­¥éª¤5åæ•°æ®é‡: {len(merged_df)} è¡Œ")
        
        # å¦‚æœæ•°æ®ä»ç„¶ä¸ºç©ºï¼Œè¿›ä¸€æ­¥é™ä½è¦æ±‚
        if len(merged_df) == 0:
            logger.warning("å†å²æ•°æ®è¿‡æ»¤åæ•°æ®ä¸ºç©ºï¼Œé™ä½æœ€å°å†å²è¦æ±‚")
            self.config['min_stock_history'] = 5
            merged_df = self.filter_stocks_by_history(self.calculate_returns_and_features(self.merge_datasets(date_range)))
            logger.info(f"é™ä½è¦æ±‚åæ•°æ®é‡: {len(merged_df)} è¡Œ")
        
        # æ¢å¤åŸå§‹é…ç½®
        self.config['min_stock_history'] = original_min_history
        
        # æœ€ç»ˆæ£€æŸ¥
        if len(merged_df) == 0:
            logger.error("æ‰€æœ‰è¿‡æ»¤æ­¥éª¤åæ•°æ®ä¸ºç©ºï¼Œæµç¨‹ç»ˆæ­¢")
            return None
        
        # 6. ç‰¹å¾æ ‡å‡†åŒ–
        merged_df = self.standardize_features(merged_df)
        logger.info(f"æ­¥éª¤6åæ•°æ®é‡: {len(merged_df)} è¡Œ")
        
        # 7. åˆ›å»ºæ—¶é—´åºåˆ—
        logger.info("å¼€å§‹åˆ›å»ºæ—¶é—´åºåˆ—...")
        try:
            sequences = self.create_sequences(merged_df)
            
            # å…³é”®æ£€æŸ¥ï¼šç¡®ä¿åºåˆ—åˆ›å»ºæˆåŠŸ
            if sequences is None:
                logger.error("âŒ æ—¶é—´åºåˆ—åˆ›å»ºå¤±è´¥ï¼šè¿”å›None")
                logger.info("è¯Šæ–­ä¿¡æ¯ï¼š")
                logger.info(f"  - åˆå¹¶æ•°æ®å½¢çŠ¶: {merged_df.shape}")
                logger.info(f"  - æ•°æ®æ—¶é—´èŒƒå›´: {merged_df['date'].min()} åˆ° {merged_df['date'].max()}")
                logger.info(f"  - å”¯ä¸€è‚¡ç¥¨æ•°: {merged_df['StockID'].nunique()}")
                logger.info(f"  - å› å­åˆ—æ•°: {len([col for col in merged_df.columns if col.startswith('Exposure_')])}")
                return None
            
            if 'factor_sequences' not in sequences or sequences['factor_sequences'] is None:
                logger.error("âŒ å› å­åºåˆ—åˆ›å»ºå¤±è´¥ï¼šfactor_sequencesä¸ºç©º")
                logger.info(f"åºåˆ—å­—å…¸keys: {list(sequences.keys()) if sequences else 'None'}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ æ—¶é—´åºåˆ—åˆ›å»ºè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            logger.info("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            import traceback
            logger.info(traceback.format_exc())
            
            # å°è¯•æ•°æ®è¯Šæ–­
            try:
                logger.info("å°è¯•æ•°æ®è´¨é‡è¯Šæ–­...")
                diagnosis = self.diagnose_data_quality(merged_df)
                logger.info(f"æ•°æ®è¯Šæ–­æ‘˜è¦:")
                logger.info(f"  - æ•°æ®è¦†ç›–ç‡: {diagnosis['data_coverage']['coverage_rate']:.2f}%")
                logger.info(f"  - ç¼ºå¤±å€¼æœ€é«˜çš„5åˆ—:")
                missing_sorted = sorted([(k, v['percentage']) for k, v in diagnosis['missing_values'].items()], 
                                      key=lambda x: x[1], reverse=True)[:5]
                for col, pct in missing_sorted:
                    logger.info(f"    {col}: {pct:.2f}%")
            except Exception as diag_e:
                logger.warning(f"æ•°æ®è¯Šæ–­ä¹Ÿå¤±è´¥: {str(diag_e)}")
            
            return None
        
        logger.info(f"åºåˆ—åˆ›å»ºæˆåŠŸï¼Œå› å­åºåˆ—å½¢çŠ¶: {sequences['factor_sequences'].shape}")
        
        # 8. åˆ›å»ºé‚»æ¥çŸ©é˜µï¼ˆGPUåŠ é€Ÿï¼‰
        if self.use_gpu and self.config.get('use_gpu_for_correlation', True):
            logger.info("ä½¿ç”¨GPUåŠ é€Ÿåˆ›å»ºé‚»æ¥çŸ©é˜µ")
            adj_matrices = self.gpu_accelerated_create_adjacency_matrices(sequences['factor_sequences'])
        else:
            logger.info("ä½¿ç”¨CPUåˆ›å»ºé‚»æ¥çŸ©é˜µ")
            adj_matrices = self.create_adjacency_matrices(sequences['factor_sequences'])
        sequences['adjacency_matrices'] = adj_matrices
        
        # 9. åˆ†å‰²æ•°æ®é›†
        split_data = self.split_data(sequences)
        
        # 10. ä¿å­˜å¤„ç†åçš„æ•°æ®
        final_data = {
            'sequences': split_data,
            'metadata': {
                'factor_names': sequences['factor_names'],
                'stock_ids': sequences['stock_ids'],
                'dates': sequences['dates'],
                'config': self.config
            }
        }
        
        self.save_processed_data(final_data)
        
        end_time = time.time()
        
        # è¾“å‡ºå¤„ç†ç»“æœæ‘˜è¦
        train_shape = split_data['train']['factor_sequences'].shape
        logger.info("\nå¤„ç†å®Œæˆæ‘˜è¦:")
        logger.info(f"æ€»å¤„ç†æ—¶é—´: {end_time - start_time:.2f} ç§’")
        logger.info(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_shape}")
        logger.info(f"å› å­æ•°é‡: {len(sequences['factor_names'])}")
        logger.info(f"è‚¡ç¥¨æ•°é‡: {len(sequences['stock_ids'])}")
        logger.info(f"æ—¶é—´åºåˆ—é•¿åº¦: {self.config['sequence_length']}")
        logger.info(f"é¢„æµ‹æ—¶é—´è·¨åº¦: {self.config['prediction_horizon']}")
        
        logger.info("\nä½¿ç”¨å»ºè®®:")
        logger.info("1. æ•°æ®å·²ä¿å­˜ä¸º 'processed_astgnn_data.pt'")
        logger.info("2. å¯ç›´æ¥ç”¨äºASTGNNæ¨¡å‹è®­ç»ƒ")
        logger.info("3. åŒ…å«å®Œæ•´çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ†å‰²")
        logger.info("4. å·²è¿›è¡Œæ ‡å‡†åŒ–å’Œå¼‚å¸¸å€¼å¤„ç†")
        
        return final_data
    
    def diagnose_data_quality(self, data: pd.DataFrame) -> Dict[str, Any]:
        """è¯Šæ–­æ•°æ®è´¨é‡"""
        logger.info("å¼€å§‹æ•°æ®è´¨é‡è¯Šæ–­")
        
        diagnosis = {
            'data_shape': data.shape,
            'date_range': (data['date'].min(), data['date'].max()),
            'unique_stocks': data['StockID'].nunique(),
            'total_stocks': len(data['StockID'].unique()),
            'missing_values': {},
            'data_coverage': {},
            'factor_analysis': {},
            'time_series_analysis': {}
        }
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        for col in data.columns:
            missing_count = data[col].isnull().sum()
            missing_pct = missing_count / len(data) * 100
            diagnosis['missing_values'][col] = {
                'count': missing_count,
                'percentage': missing_pct
            }
        
        # æ£€æŸ¥å› å­åˆ—
        factor_cols = [col for col in data.columns if col.startswith('Exposure_')]
        diagnosis['factor_analysis'] = {
            'factor_count': len(factor_cols),
            'factor_names': factor_cols,
            'factor_stats': {}
        }
        
        # å› å­ç»Ÿè®¡ä¿¡æ¯
        for factor in factor_cols:
            factor_data = data[factor].dropna()
            diagnosis['factor_analysis']['factor_stats'][factor] = {
                'mean': factor_data.mean(),
                'std': factor_data.std(),
                'min': factor_data.min(),
                'max': factor_data.max(),
                'skewness': factor_data.skew(),
                'kurtosis': factor_data.kurtosis()
            }
        
        # æ—¶é—´åºåˆ—åˆ†æ
        dates = sorted(data['date'].unique())
        diagnosis['time_series_analysis'] = {
            'total_dates': len(dates),
            'date_gaps': self._find_date_gaps(dates),
            'stocks_per_date': data.groupby('date')['StockID'].nunique().describe().to_dict(),
            'observations_per_stock': data.groupby('StockID').size().describe().to_dict()
        }
        
        # æ•°æ®è¦†ç›–ç‡åˆ†æ
        total_expected = len(dates) * diagnosis['unique_stocks']
        actual_observations = len(data)
        diagnosis['data_coverage'] = {
            'expected_observations': total_expected,
            'actual_observations': actual_observations,
            'coverage_rate': actual_observations / total_expected * 100 if total_expected > 0 else 0
        }
        
        logger.info("æ•°æ®è´¨é‡è¯Šæ–­å®Œæˆ")
        return diagnosis
    
    def _find_date_gaps(self, dates: List) -> List[Tuple]:
        """æŸ¥æ‰¾æ—¥æœŸé—´éš™"""
        gaps = []
        dates = pd.to_datetime(dates)
        
        for i in range(1, len(dates)):
            gap_days = (dates[i] - dates[i-1]).days
            if gap_days > 7:  # å‡è®¾æ­£å¸¸é—´éš”ä¸è¶…è¿‡7å¤©
                gaps.append((dates[i-1], dates[i], gap_days))
        
        return gaps
    
    def validate_data_consistency(self, stock_data: pd.DataFrame, barra_data: pd.DataFrame) -> Dict[str, Any]:
        """éªŒè¯ä¸¤ä¸ªæ•°æ®é›†çš„ä¸€è‡´æ€§"""
        logger.info("éªŒè¯æ•°æ®ä¸€è‡´æ€§")
        
        validation = {
            'stock_date_range': (stock_data['date'].min(), stock_data['date'].max()),
            'barra_date_range': (barra_data['date'].min(), barra_data['date'].max()),
            'common_stocks': set(stock_data['StockID']).intersection(set(barra_data['StockID'])),
            'stock_only': set(stock_data['StockID']) - set(barra_data['StockID']),
            'barra_only': set(barra_data['StockID']) - set(stock_data['StockID']),
            'overlap_analysis': {}
        }
        
        # åˆ†æé‡å æƒ…å†µ
        common_stocks = validation['common_stocks']
        validation['overlap_analysis'] = {
            'common_stock_count': len(common_stocks),
            'stock_only_count': len(validation['stock_only']),
            'barra_only_count': len(validation['barra_only']),
            'overlap_rate': len(common_stocks) / max(len(set(stock_data['StockID'])), len(set(barra_data['StockID']))) * 100
        }
        
        # æ—¥æœŸé‡å åˆ†æ
        stock_dates = set(stock_data['date'])
        barra_dates = set(barra_data['date'])
        common_dates = stock_dates.intersection(barra_dates)
        
        validation['date_overlap'] = {
            'common_dates': len(common_dates),
            'stock_only_dates': len(stock_dates - barra_dates),
            'barra_only_dates': len(barra_dates - stock_dates),
            'date_overlap_rate': len(common_dates) / max(len(stock_dates), len(barra_dates)) * 100
        }
        
        logger.info("æ•°æ®ä¸€è‡´æ€§éªŒè¯å®Œæˆ")
        return validation
    
    def print_diagnosis_report(self, diagnosis: Dict[str, Any]):
        """æ‰“å°è¯Šæ–­æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("æ•°æ®è´¨é‡è¯Šæ–­æŠ¥å‘Š")
        print("="*60)
        
        print(f"æ•°æ®å½¢çŠ¶: {diagnosis['data_shape']}")
        print(f"æ—¶é—´èŒƒå›´: {diagnosis['date_range'][0]} åˆ° {diagnosis['date_range'][1]}")
        print(f"è‚¡ç¥¨æ•°é‡: {diagnosis['unique_stocks']}")
        print(f"å› å­æ•°é‡: {diagnosis['factor_analysis']['factor_count']}")
        
        print(f"\næ•°æ®è¦†ç›–ç‡: {diagnosis['data_coverage']['coverage_rate']:.2f}%")
        print(f"æœŸæœ›è§‚æµ‹æ•°: {diagnosis['data_coverage']['expected_observations']:,}")
        print(f"å®é™…è§‚æµ‹æ•°: {diagnosis['data_coverage']['actual_observations']:,}")
        
        print(f"\næ—¶é—´åºåˆ—åˆ†æ:")
        print(f"  æ€»æ—¥æœŸæ•°: {diagnosis['time_series_analysis']['total_dates']}")
        print(f"  æ—¥æœŸé—´éš™: {len(diagnosis['time_series_analysis']['date_gaps'])} ä¸ª")
        
        print(f"\nç¼ºå¤±å€¼æƒ…å†µ:")
        high_missing_cols = [(k, v['percentage']) for k, v in diagnosis['missing_values'].items() 
                           if v['percentage'] > 10]
        if high_missing_cols:
            for col, pct in high_missing_cols[:5]:  # æ˜¾ç¤ºå‰5ä¸ªé«˜ç¼ºå¤±ç‡åˆ—
                print(f"  {col}: {pct:.2f}%")
        else:
            print("  æ‰€æœ‰åˆ—ç¼ºå¤±ç‡éƒ½ä½äº10%")
        
        print("="*60)

    def gpu_accelerated_calculate_returns_and_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """GPUåŠ é€Ÿç‰ˆæœ¬çš„æ”¶ç›Šç‡å’ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—"""
        logger.info("GPUåŠ é€Ÿè®¡ç®—æ”¶ç›Šç‡å’ŒæŠ€æœ¯æŒ‡æ ‡")
        
        if not self.use_gpu or len(data) < 10000:
            return self.calculate_returns_and_features(data)
        
        data = data.reset_index(drop=True)
        data = data.sort_values(['StockID', 'date'])
        
        results = []
        stock_groups = data.groupby('StockID')
        
        # æ‰¹å¤„ç†è‚¡ç¥¨
        stock_list = list(stock_groups.groups.keys())
        batch_size = min(self.gpu_batch_size // 10, len(stock_list))  # è°ƒæ•´æ‰¹é‡å¤§å°
        
        for i in range(0, len(stock_list), batch_size):
            batch_stocks = stock_list[i:i+batch_size]
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡çš„è‚¡ç¥¨
            for stock_id in batch_stocks:
                stock_data = stock_groups.get_group(stock_id).copy()
                if len(stock_data) < 2:
                    continue
                
                # è½¬æ¢åˆ°GPUè¿›è¡Œè®¡ç®—
                stock_data_gpu = self._gpu_process_single_stock(stock_data)
                results.append(stock_data_gpu)
            
            # å®šæœŸæ¸…ç†GPUå†…å­˜
            if i % (batch_size * 5) == 0:
                torch.cuda.empty_cache()
        
        final_data = pd.concat(results, ignore_index=True) if results else data
        logger.info("GPUåŠ é€Ÿæ”¶ç›Šç‡å’ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆ")
        return final_data

    def _gpu_process_single_stock(self, stock_data: pd.DataFrame) -> pd.DataFrame:
        """ä½¿ç”¨GPUå¤„ç†å•åªè‚¡ç¥¨çš„æŠ€æœ¯æŒ‡æ ‡"""
        # æå–ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®åˆ°GPU
        prices = torch.tensor(stock_data['close'].values, device=self.device, dtype=torch.float32)
        volumes = torch.tensor(stock_data['vol'].values, device=self.device, dtype=torch.float32)
        
        # GPUè®¡ç®—å„ç§æ”¶ç›Šç‡
        stock_data['return_1d'] = self._gpu_pct_change(prices, 1).cpu().numpy()
        stock_data['return_5d'] = self._gpu_pct_change(prices, 5).cpu().numpy()
        stock_data['return_10d'] = self._gpu_pct_change(prices, 10).cpu().numpy()
        stock_data['return_20d'] = self._gpu_pct_change(prices, 20).cpu().numpy()
        
        # å¯¹æ•°æ”¶ç›Šç‡
        log_returns = torch.cat([
            torch.tensor([0.0], device=self.device),
            torch.log(prices[1:] / prices[:-1])
        ])
        stock_data['log_return'] = log_returns.cpu().numpy()
        
        # æ³¢åŠ¨ç‡æŒ‡æ ‡
        stock_data['volatility_5d'] = self._gpu_rolling_std(log_returns, 5).cpu().numpy()
        stock_data['volatility_20d'] = self._gpu_rolling_std(log_returns, 20).cpu().numpy()
        
        # ç§»åŠ¨å¹³å‡çº¿
        ma_5 = self._gpu_rolling_mean(prices, 5)
        ma_20 = self._gpu_rolling_mean(prices, 20)
        vol_ma_5 = self._gpu_rolling_mean(volumes, 5)
        
        stock_data['price_ma_5'] = ma_5.cpu().numpy()
        stock_data['price_ma_20'] = ma_20.cpu().numpy()
        stock_data['price_ratio_ma5'] = (prices / ma_5).cpu().numpy()
        stock_data['price_ratio_ma20'] = (prices / ma_20).cpu().numpy()
        stock_data['volume_ma_5'] = vol_ma_5.cpu().numpy()
        stock_data['volume_ratio'] = (volumes / vol_ma_5).cpu().numpy()
        
        # åŠ¨é‡æŒ‡æ ‡
        stock_data['momentum_5d'] = self._gpu_momentum(prices, 5).cpu().numpy()
        stock_data['momentum_20d'] = self._gpu_momentum(prices, 20).cpu().numpy()
        
        return stock_data

    def _gpu_pct_change(self, prices: torch.Tensor, periods: int) -> torch.Tensor:
        """GPUç‰ˆæœ¬çš„ç™¾åˆ†æ¯”å˜åŒ–è®¡ç®—"""
        if len(prices) <= periods:
            return torch.zeros_like(prices)
        
        shifted = torch.roll(prices, periods)
        shifted[:periods] = prices[0]  # å¡«å……å‰å‡ æœŸ
        returns = (prices - shifted) / shifted
        returns[:periods] = 0.0  # å‰å‡ æœŸè®¾ä¸º0
        return returns

    def _gpu_rolling_mean(self, data: torch.Tensor, window: int) -> torch.Tensor:
        """GPUç‰ˆæœ¬çš„æ»šåŠ¨å¹³å‡è®¡ç®—"""
        if len(data) < window:
            return torch.full_like(data, data.mean())
        
        # ä½¿ç”¨1Då·ç§¯å®ç°æ»šåŠ¨å¹³å‡
        data_expanded = data.unsqueeze(0).unsqueeze(0)  # [1, 1, length]
        kernel = torch.ones(1, 1, window, device=self.device) / window
        
        # å·¦å¡«å……ä»¥ä¿æŒé•¿åº¦
        padded = torch.nn.functional.pad(data_expanded, (window-1, 0))
        result = torch.nn.functional.conv1d(padded, kernel)
        
        return result.squeeze()

    def _gpu_rolling_std(self, data: torch.Tensor, window: int) -> torch.Tensor:
        """GPUç‰ˆæœ¬çš„æ»šåŠ¨æ ‡å‡†å·®è®¡ç®—"""
        if len(data) < window:
            return torch.full_like(data, data.std())
        
        # è®¡ç®—æ»šåŠ¨å‡å€¼
        mean_rolled = self._gpu_rolling_mean(data, window)
        
        # è®¡ç®—å¹³æ–¹çš„æ»šåŠ¨å¹³å‡
        data_squared = data ** 2
        mean_squared = self._gpu_rolling_mean(data_squared, window)
        
        # æ–¹å·® = E[XÂ²] - E[X]Â²
        variance = mean_squared - mean_rolled ** 2
        variance = torch.clamp(variance, min=0)  # ç¡®ä¿éè´Ÿ
        
        return torch.sqrt(variance)

    def _gpu_momentum(self, prices: torch.Tensor, periods: int) -> torch.Tensor:
        """GPUç‰ˆæœ¬çš„åŠ¨é‡è®¡ç®—"""
        if len(prices) <= periods:
            return torch.zeros_like(prices)
        
        shifted = torch.roll(prices, periods)
        shifted[:periods] = prices[0]
        momentum = (prices / shifted) - 1
        momentum[:periods] = 0.0
        return momentum

    def gpu_accelerated_create_adjacency_matrices(self, factor_sequences: torch.Tensor) -> torch.Tensor:
        """GPUåŠ é€Ÿç‰ˆæœ¬çš„é‚»æ¥çŸ©é˜µåˆ›å»º - å¤§è§„æ¨¡ä¼˜åŒ–"""
        logger.info("GPUåŠ é€Ÿåˆ›å»ºé‚»æ¥çŸ©é˜µï¼ˆå¤§è§„æ¨¡ä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
        logger.info(f"factor_sequenceså½¢çŠ¶: {factor_sequences.shape}")
        
        if not self.use_gpu or not self.config.get('use_gpu_for_correlation', True):
            return self.create_adjacency_matrices(factor_sequences)
        
        # æ£€æŸ¥ç»´åº¦å¹¶è¿›è¡Œé€‚å½“çš„å¤„ç†
        if len(factor_sequences.shape) == 3:
            num_sequences, seq_len, num_features = factor_sequences.shape
            logger.warning(f"GPUé‚»æ¥çŸ©é˜µ: æ¥æ”¶åˆ°3ç»´factor_sequences: {factor_sequences.shape}")
            
            # è‡ªåŠ¨æ¨æ–­åˆç†çš„è‚¡ç¥¨æ•°å’Œå› å­æ•°
            num_factors = min(25, num_features)
            num_stocks = max(1, num_features // num_factors)
            
            # å®Œå…¨ç§»é™¤ç¡¬ç¼–ç é™åˆ¶ï¼Œæ”¯æŒå¤§è§„æ¨¡å¤„ç†
            max_sequences = num_sequences  # ç§»é™¤åºåˆ—æ•°é‡é™åˆ¶ï¼Œå®Œå…¨åŸºäºå†…å­˜åŠ¨æ€è®¡ç®—
            max_stocks = num_stocks  # å®Œå…¨ç§»é™¤è‚¡ç¥¨æ•°é‡é™åˆ¶
            
            try:
                # é‡å¡‘ä¸º4ç»´
                factor_sequences_truncated = factor_sequences[:max_sequences, :, :max_stocks*num_factors]
                factor_sequences = factor_sequences_truncated.view(max_sequences, seq_len, max_stocks, num_factors)
                num_sequences, seq_len, num_stocks, num_factors = factor_sequences.shape
                logger.info(f"GPUæˆåŠŸé‡å¡‘ä¸º4ç»´: {factor_sequences.shape}")
            except Exception as e:
                logger.error(f"GPU 4ç»´é‡å¡‘å¤±è´¥: {str(e)}")
                # åˆ›å»ºé»˜è®¤å¼ é‡ - ä¿æŒåŸå§‹è‚¡ç¥¨æ•°é‡
                num_sequences, seq_len, num_stocks, num_factors = 10, factor_sequences.shape[1], max_stocks, 10
                factor_sequences = torch.randn(num_sequences, seq_len, num_stocks, num_factors)
        elif len(factor_sequences.shape) == 4:
            num_sequences, seq_len, num_stocks, num_factors = factor_sequences.shape
            logger.info(f"GPUæ¥æ”¶åˆ°æ­£ç¡®çš„4ç»´factor_sequences: {factor_sequences.shape}")
        else:
            logger.error(f"GPUä¸æ”¯æŒçš„factor_sequencesç»´åº¦: {factor_sequences.shape}")
            return torch.eye(10).unsqueeze(0).unsqueeze(0).expand(1, 1, -1, -1)
        
        # è®¡ç®—é‚»æ¥çŸ©é˜µå†…å­˜éœ€æ±‚
        adj_matrix_memory_gb = (num_sequences * seq_len * num_stocks * num_stocks * 4) / (1024**3)
        logger.info(f"é‚»æ¥çŸ©é˜µå†…å­˜éœ€æ±‚: {adj_matrix_memory_gb:.2f} GB (è‚¡ç¥¨æ•°: {num_stocks})")
        
        # é‡‡ç”¨åˆ†çº§å¤„ç†ç­–ç•¥
        if adj_matrix_memory_gb > 20.0:
            logger.warning(f"é‚»æ¥çŸ©é˜µå†…å­˜éœ€æ±‚è¿‡å¤§({adj_matrix_memory_gb:.2f}GB > 20GB)ï¼Œä½¿ç”¨ç¨€ç–èº«ä»½çŸ©é˜µ")
            # å¯¹äºè¶…å¤§è§„æ¨¡æ•°æ®ï¼Œè¿”å›ç¨€ç–èº«ä»½çŸ©é˜µ
            identity_matrix = torch.eye(num_stocks).unsqueeze(0).unsqueeze(0).expand(num_sequences, seq_len, -1, -1)
            return identity_matrix
        elif adj_matrix_memory_gb > 8.0:
            logger.info(f"é‡‡ç”¨åˆ†å—å¤„ç†ç­–ç•¥å¤„ç†å¤§è§„æ¨¡é‚»æ¥çŸ©é˜µ({adj_matrix_memory_gb:.2f}GB)")
            return self._create_large_scale_adjacency_matrices(factor_sequences)
        else:
            logger.info(f"ä½¿ç”¨æ ‡å‡†GPUå¤„ç†({adj_matrix_memory_gb:.2f}GB)")
            return self._create_standard_adjacency_matrices(factor_sequences)

    def _create_large_scale_adjacency_matrices(self, factor_sequences: torch.Tensor) -> torch.Tensor:
        """åˆ†å—å¤„ç†å¤§è§„æ¨¡é‚»æ¥çŸ©é˜µ"""
        num_sequences, seq_len, num_stocks, num_factors = factor_sequences.shape
        logger.info(f"åˆ†å—å¤„ç†å¤§è§„æ¨¡é‚»æ¥çŸ©é˜µ: {num_stocks}åªè‚¡ç¥¨")
        
        # è®¡ç®—åˆé€‚çš„åˆ†å—å¤§å°
        gpu_memory = self.get_gpu_memory_usage()
        available_gb = gpu_memory.get('free_gb', 8)
        block_size = min(500, int(np.sqrt(available_gb * 1024**3 / (seq_len * 4))))  # åŠ¨æ€åˆ†å—å¤§å°
        block_size = max(50, block_size)  # æœ€å°åˆ†å—å¤§å°
        
        logger.info(f"ä½¿ç”¨åˆ†å—å¤§å°: {block_size}, å¯ç”¨å†…å­˜: {available_gb:.2f}GB")
        
        # åˆå§‹åŒ–ç»“æœå¼ é‡
        adj_matrices = torch.zeros(num_sequences, seq_len, num_stocks, num_stocks)
        threshold = self.config['adjacency_threshold']
        
        # ç§»åŠ¨åˆ°GPU
        factor_sequences_gpu = factor_sequences.to(self.device)
        
        # åˆ†å—å¤„ç†
        for seq_idx in range(num_sequences):
            for t in range(seq_len):
                factors_t = factor_sequences_gpu[seq_idx, t]  # [stocks, factors]
                
                if torch.isnan(factors_t).all():
                    adj_matrices[seq_idx, t] = torch.eye(num_stocks)
                    continue
                
                # æ ‡å‡†åŒ–ç‰¹å¾
                factors_norm = torch.nn.functional.normalize(factors_t, dim=1, eps=1e-8)
                
                # åˆ†å—è®¡ç®—ç›¸å…³çŸ©é˜µ
                adj_matrix_gpu = torch.zeros(num_stocks, num_stocks, device=self.device)
                
                for i in range(0, num_stocks, block_size):
                    i_end = min(i + block_size, num_stocks)
                    for j in range(0, num_stocks, block_size):
                        j_end = min(j + block_size, num_stocks)
                        
                        # è®¡ç®—å—ç›¸å…³çŸ©é˜µ
                        block_corr = torch.mm(factors_norm[i:i_end], factors_norm[j:j_end].t())
                        block_corr = torch.nan_to_num(block_corr, nan=0.0)
                        
                        # åº”ç”¨é˜ˆå€¼
                        block_adj = torch.abs(block_corr)
                        block_adj[block_adj < threshold] = 0
                        
                        adj_matrix_gpu[i:i_end, j:j_end] = block_adj
                
                # è®¾ç½®å¯¹è§’çº¿ä¸º1
                adj_matrix_gpu.fill_diagonal_(1.0)
                adj_matrices[seq_idx, t] = adj_matrix_gpu.cpu()
                
                # å®šæœŸæ¸…ç†GPUå†…å­˜
                if (seq_idx * seq_len + t) % 50 == 0:
                    torch.cuda.empty_cache()
        
        logger.info(f"åˆ†å—é‚»æ¥çŸ©é˜µåˆ›å»ºå®Œæˆ: {adj_matrices.shape}")
        return adj_matrices

    def _create_standard_adjacency_matrices(self, factor_sequences: torch.Tensor) -> torch.Tensor:
        """æ ‡å‡†GPUé‚»æ¥çŸ©é˜µåˆ›å»º"""
        num_sequences, seq_len, num_stocks, num_factors = factor_sequences.shape
        
        # ç§»åŠ¨åˆ°GPU
        factor_sequences_gpu = factor_sequences.to(self.device)
        adj_matrices = torch.zeros(num_sequences, seq_len, num_stocks, num_stocks, device=self.device)
        threshold = self.config['adjacency_threshold']
        
        # æ‰¹é‡å¤„ç†ä»¥èŠ‚çœGPUå†…å­˜
        batch_size = min(20, num_sequences)  # å‡å°æ‰¹æ¬¡å¤§å°ä»¥èŠ‚çœå†…å­˜
        
        for batch_start in range(0, num_sequences, batch_size):
            batch_end = min(batch_start + batch_size, num_sequences)
            
            for seq_idx in range(batch_start, batch_end):
                for t in range(seq_len):
                    factors_t = factor_sequences_gpu[seq_idx, t]  # [stocks, factors]
                    
                    if not torch.isnan(factors_t).all() and factors_t.shape[0] > 1:
                        try:
                            # æ ‡å‡†åŒ–ç‰¹å¾
                            factors_norm = torch.nn.functional.normalize(factors_t, dim=1, eps=1e-8)
                            
                            # è®¡ç®—ç›¸å…³çŸ©é˜µ (GPUå¹¶è¡Œ)
                            corr_matrix = torch.mm(factors_norm, factors_norm.t())
                            corr_matrix = torch.nan_to_num(corr_matrix, nan=0.0)
                            
                            # è½¬æ¢ä¸ºé‚»æ¥çŸ©é˜µ
                            adj_matrix = torch.abs(corr_matrix)
                            adj_matrix[adj_matrix < threshold] = 0
                            adj_matrix.fill_diagonal_(1.0)
                            
                            adj_matrices[seq_idx, t] = adj_matrix
                        except Exception as e:
                            logger.warning(f"GPUç›¸å…³çŸ©é˜µè®¡ç®—å¤±è´¥ seq_idx={seq_idx}, t={t}: {str(e)}")
                            adj_matrices[seq_idx, t] = torch.eye(num_stocks, device=self.device)
                    else:
                        adj_matrices[seq_idx, t] = torch.eye(num_stocks, device=self.device)
            
            # å®šæœŸæ¸…ç†GPUå†…å­˜
            if batch_start % (batch_size * 5) == 0:
                torch.cuda.empty_cache()
        
        # ç§»å›CPU
        adj_matrices_cpu = adj_matrices.cpu()
        torch.cuda.empty_cache()
        
        logger.info(f"æ ‡å‡†GPUé‚»æ¥çŸ©é˜µåˆ›å»ºå®Œæˆ: {adj_matrices_cpu.shape}")
        return adj_matrices_cpu

    def get_gpu_memory_usage(self) -> Dict[str, float]:
        """è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if not self.use_gpu:
            return {"message": "GPUæœªå¯ç”¨", "used_gb": 0.0, "total_gb": 0.0}
        
        try:
            allocated_gb = torch.cuda.memory_allocated() / 1e9
            cached_gb = torch.cuda.memory_reserved() / 1e9
            total_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
            used_gb = max(allocated_gb, cached_gb)  # ä½¿ç”¨å·²åˆ†é…æˆ–ç¼“å­˜çš„è¾ƒå¤§å€¼
            
            memory_info = {
                'allocated_gb': allocated_gb,
                'cached_gb': cached_gb,
                'used_gb': used_gb,
                'total_gb': total_gb,
                'free_gb': total_gb - used_gb,
                'utilization_pct': (used_gb / total_gb) * 100 if total_gb > 0 else 0
            }
            
            return memory_info
        except Exception as e:
            logger.warning(f"è·å–GPUå†…å­˜ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"message": "GPUä¿¡æ¯è·å–å¤±è´¥", "used_gb": 0.0, "total_gb": 0.0}

    def _force_cleanup_gpu_memory(self):
        """å¼ºåˆ¶æ¸…ç†GPUå†…å­˜ - è§£å†³å†…å­˜æ³„æ¼é—®é¢˜"""
        if not self.use_gpu:
            return
            
        logger.warning("ğŸ§¹ æ‰§è¡Œå¼ºåˆ¶GPUå†…å­˜æ¸…ç†...")
        
        try:
            # 1. å¤šæ¬¡æ¸…ç©ºç¼“å­˜
            for i in range(5):
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
            # 2. å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            for i in range(3):
                gc.collect()
                torch.cuda.empty_cache()
            
            # 3. é‡ç½®å†…å­˜ç»Ÿè®¡
            torch.cuda.reset_peak_memory_stats()
            
            # 4. æ£€æŸ¥æ¸…ç†æ•ˆæœ
            memory_info = self.get_gpu_memory_usage()
            logger.info(f"GPUå†…å­˜æ¸…ç†å®Œæˆ: {memory_info.get('used_gb', 0):.2f}GB ä½¿ç”¨ä¸­, {memory_info.get('free_gb', 0):.2f}GB å¯ç”¨")
            
            # 5. å¦‚æœå†…å­˜ä»ç„¶å¾ˆå°‘ï¼Œå»ºè®®é‡å¯
            if memory_info.get('free_gb', 0) < 3.0:
                logger.error("âŒ GPUå†…å­˜ä¸¥é‡ä¸è¶³ï¼å»ºè®®é‡å¯Pythonè¿›ç¨‹é‡Šæ”¾å†…å­˜ç¢ç‰‡")
                
        except Exception as e:
            logger.error(f"GPUå†…å­˜æ¸…ç†å¤±è´¥: {str(e)}")

    def optimize_gpu_memory(self):
        """ä¼˜åŒ–GPUå†…å­˜ä½¿ç”¨"""
        if self.use_gpu:
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            logger.info("GPUå†…å­˜ä¼˜åŒ–å®Œæˆ")

    def set_gpu_batch_size(self, batch_size: int):
        """åŠ¨æ€è°ƒæ•´GPUæ‰¹å¤„ç†å¤§å°"""
        self.gpu_batch_size = batch_size
        logger.info(f"GPUæ‰¹å¤„ç†å¤§å°å·²è®¾ç½®ä¸º: {batch_size}")

    def benchmark_gpu_performance(self, test_data_size: int = 50000) -> Dict[str, float]:
        """GPUæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info(f"å¼€å§‹GPUæ€§èƒ½åŸºå‡†æµ‹è¯• - æ•°æ®è§„æ¨¡: {test_data_size}")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        num_stocks = 200
        num_periods = test_data_size // num_stocks
        
        test_data = pd.DataFrame({
            'StockID': np.repeat(range(num_stocks), num_periods),
            'date': pd.date_range('2020-01-01', periods=num_periods).repeat(num_stocks),
            'close': np.random.randn(test_data_size) * 100 + 1000,
            'vol': np.random.randint(1000, 100000, test_data_size)
        })
        
        results = {}
        
        # æµ‹è¯•CPUæ€§èƒ½
        original_use_gpu = self.use_gpu
        self.use_gpu = False
        
        start_time = time.time()
        cpu_result = self.calculate_returns_and_features(test_data.copy())
        results['cpu_time'] = time.time() - start_time
        
        # æµ‹è¯•GPUæ€§èƒ½
        if torch.cuda.is_available():
            self.use_gpu = True
            torch.cuda.reset_peak_memory_stats()
            
            start_time = time.time()
            gpu_result = self.gpu_accelerated_calculate_returns_and_features(test_data.copy())
            results['gpu_time'] = time.time() - start_time
            
            if results['gpu_time'] > 0:
                results['speedup'] = results['cpu_time'] / results['gpu_time']
            
            # GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            memory_info = self.get_gpu_memory_usage()
            results.update(memory_info)
        
        # æ¢å¤åŸå§‹è®¾ç½®
        self.use_gpu = original_use_gpu
        
        logger.info(f"æ€§èƒ½æµ‹è¯•å®Œæˆ: {results}")
        return results


def run_stock_friendly_preprocessing():
    """è¿è¡Œè‚¡ç¥¨å‹å¥½çš„æ•°æ®é¢„å¤„ç†æµç¨‹ - ä¿ç•™æ›´å¤šè‚¡ç¥¨"""
    logger.info("="*80)
    logger.info("å¯åŠ¨è‚¡ç¥¨å‹å¥½æ•°æ®é¢„å¤„ç†æµç¨‹ - ç›®æ ‡ï¼šæœ€å¤§åŒ–ä¿ç•™è‚¡ç¥¨æ•°é‡")
    logger.info("="*80)
    
    # åˆ›å»ºè‚¡ç¥¨å‹å¥½çš„é¢„å¤„ç†å™¨
    preprocessor = ASTGNNDataPreprocessor()
    
    # è®¾ç½®è‚¡ç¥¨å‹å¥½é…ç½®
    stock_friendly_config = {
        'min_stock_history': 60,        # æ›´å®½æ¾ï¼š60å¤© vs 252å¤©
        'outlier_threshold': 10.0,      # æ›´å®½æ¾ï¼š10.0 vs 5.0
        'outlier_method': 'stock_wise', # æŒ‰è‚¡ç¥¨å¤„ç†
        'outlier_winsorize': True,      # ç¼©å°¾è€Œéåˆ é™¤
        'max_missing_ratio': 0.4,       # å…è®¸40%ç¼ºå¤±
        'min_trading_days': 40,         # æœ€å°‘40ä¸ªäº¤æ˜“æ—¥
        'adjacency_threshold': 0.08,    # æ›´åŒ…å®¹çš„é‚»æ¥é˜ˆå€¼
    }
    
    # æ›´æ–°é…ç½®
    preprocessor.config.update(stock_friendly_config)
    
    try:
        # è¿è¡Œé¢„å¤„ç†æµç¨‹ - æ‰©å±•åˆ°ä¸€å¹´æ•°æ®èŒƒå›´ï¼ˆæä¾›æ›´å¤šè®­ç»ƒæ•°æ®ï¼‰
        result = preprocessor.run_preprocessing_pipeline(
            stock_sample_size=None,  # ç¦ç”¨é‡‡æ ·ï¼Œä¿ç•™æ‰€æœ‰è‚¡ç¥¨
            barra_sample_size=None,  # ç¦ç”¨é‡‡æ ·ï¼Œä¿ç•™æ‰€æœ‰è‚¡ç¥¨
            date_range=('2023-01-01', '2023-12-31')  # æ‰©å±•åˆ°ä¸€æ•´å¹´ï¼ˆçº¦250ä¸ªäº¤æ˜“æ—¥ï¼‰
        )
        
        if result:
            logger.info("è‚¡ç¥¨å‹å¥½é¢„å¤„ç†å®Œæˆï¼")
            logger.info(f"æœ€ç»ˆä¿ç•™è‚¡ç¥¨æ•°é‡: {result.get('final_stock_count', 'N/A')}")
            logger.info(f"é¢„æœŸæ”¹å–„: ç›¸æ¯”åŸé…ç½®å¯å¤šä¿ç•™çº¦10-20åªè‚¡ç¥¨")
            return result
        else:
            logger.error("è‚¡ç¥¨å‹å¥½é¢„å¤„ç†å¤±è´¥")
            return None
            
    except Exception as e:
        logger.error(f"é¢„å¤„ç†è¿‡ç¨‹å‡ºé”™: {str(e)}")
        return None


def main():
    """ä¸»å‡½æ•° - è‡ªåŠ¨è¿è¡Œè‚¡ç¥¨å‹å¥½çš„æ•°æ®é¢„å¤„ç†"""
    print("\n" + "="*80)
    print("ASTGNNæ•°æ®é¢„å¤„ç†å™¨ - æœ€å¤§åŒ–è‚¡ç¥¨ä¿ç•™ç³»ç»Ÿ")
    print("="*80)
    print("è‡ªåŠ¨è¿è¡Œè‚¡ç¥¨å‹å¥½é¢„å¤„ç†æ¨¡å¼ï¼Œæœ€å¤§åŒ–ä¿ç•™è‚¡ç¥¨æ•°é‡")
    print("\nä¼˜åŒ–ç‰¹æ€§ï¼š")
    print("  * å®Œå…¨ç¦ç”¨é‡‡æ ·é™åˆ¶ï¼Œä¿ç•™æ‰€æœ‰è‚¡ç¥¨")
    print("  * å†å²æ•°æ®è¦æ±‚ï¼š60å¤©ï¼ˆæ›´ç°å®ï¼‰")
    print("  * å¼‚å¸¸å€¼é˜ˆå€¼ï¼š10.0ï¼ˆæ›´å®½æ¾ï¼‰")
    print("  * æŒ‰è‚¡ç¥¨åˆ†ç»„å¤„ç†ï¼ˆé¿å…å…¨å±€åˆ é™¤ï¼‰")
    print("  * ç¼©å°¾å¤„ç†ä»£æ›¿åˆ é™¤ï¼ˆä¿ç•™æ›´å¤šæ•°æ®ï¼‰")
    print("="*80)
    
    start_time = time.time()
    
    print("\nå¯åŠ¨è‚¡ç¥¨å‹å¥½é¢„å¤„ç†æ¨¡å¼...")
    print("è¿™å°†éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    result = run_stock_friendly_preprocessing()
    
    if result:
        end_time = time.time()
        processing_time = end_time - start_time
        
        print("\n" + "="*60)
        print("æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
        print("="*60)
        print(f"æœ€ç»ˆç»Ÿè®¡:")
        print(f"   æ•°æ®æ—¶é—´èŒƒå›´: 2023-01-01 è‡³ 2023-12-31ï¼ˆå®Œæ•´ä¸€å¹´ï¼‰")
        print(f"   äº¤æ˜“æ—¥æ•°é‡: çº¦250ä¸ªäº¤æ˜“æ—¥")
        if 'sequences' in result:
            print(f"   æ€»åºåˆ—æ•°: {result['sequences']['train']['factor_sequences'].shape[0]}")
            print(f"   è‚¡ç¥¨æ•°é‡: {len(result['metadata']['stock_ids'])}")
            print(f"   å› å­æ•°é‡: {len(result['metadata']['factor_names'])}")
        print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        print(f"\nå¯ç”¨äºASTGNNè®­ç»ƒçš„æ•°æ®å·²ä¿å­˜ä¸º 'processed_astgnn_data.pt'")
        
        print(f"\nè‚¡ç¥¨å‹å¥½æ¨¡å¼çš„ä¼˜åŠ¿ï¼š")
        print(f"   * ä»5000+åªè‚¡ç¥¨ä¸­æœ€å¤§åŒ–ä¿ç•™æœ‰æ•ˆè‚¡ç¥¨")
        print(f"   * æ•°æ®åˆ©ç”¨ç‡å¤§å¹…æå‡")
        print(f"   * å¼‚å¸¸å€¼å¤„ç†æ›´åŠ æ™ºèƒ½")
        print(f"   * å®Œå…¨ç¦ç”¨é‡‡æ ·é™åˆ¶")
        
        return result
    else:
        print("\næ•°æ®é¢„å¤„ç†å¤±è´¥")
        return None


if __name__ == '__main__':
    main() 