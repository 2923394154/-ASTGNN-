#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ASTGNNå› å­æŸå¤±å‡½æ•° - ä¸“æ³¨RankICä¼˜åŒ–ç‰ˆæœ¬
ç›´æ¥ä¼˜åŒ–RankICæŒ‡æ ‡ï¼Œç¡®ä¿å› å­é¢„æµ‹æ–¹å‘æ­£ç¡®
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import List, Optional

class ASTGNNFactorLoss(nn.Module):
    """
    ASTGNNå› å­æŸå¤±å‡½æ•° - RankICå¯¼å‘ç‰ˆæœ¬
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. ç›´æ¥ä¼˜åŒ–RankICç›¸å…³æ€§
    2. ç¡®ä¿å› å­é¢„æµ‹æ–¹å‘æ­£ç¡®
    3. é˜²æ­¢å› å­åˆ†å¸ƒåæ–œ
    4. æ§åˆ¶å› å­æ–¹å·®ç¨³å®šæ€§
    """
    
    def __init__(self, 
                 omega: float = 0.9,
                 lambda_orthogonal: float = 0.01,
                 lambda_rank_ic: float = 5.0,          # RankICæŸå¤±æƒé‡
                 lambda_distribution: float = 1.0,      # åˆ†å¸ƒæ­£åˆ™åŒ–æƒé‡
                 lambda_variance: float = 0.5,          # æ–¹å·®ç¨³å®šæ€§æƒé‡
                 max_periods: int = 3,
                 eps: float = 1e-6,
                 regularization_type: str = 'frobenius'):
        super().__init__()
        
        self.omega = omega
        self.lambda_orthogonal = lambda_orthogonal
        self.lambda_rank_ic = lambda_rank_ic
        self.lambda_distribution = lambda_distribution
        self.lambda_variance = lambda_variance
        self.max_periods = max_periods
        self.eps = eps
        self.regularization_type = regularization_type
        
        print(f"ğŸ¯ RankICå¯¼å‘æŸå¤±å‡½æ•°åˆå§‹åŒ–:")
        print(f"  Ï‰æ—¶é—´æƒé‡: {omega}")
        print(f"  Î»æ­£äº¤æƒ©ç½š: {lambda_orthogonal}")
        print(f"  Î»RankICæƒé‡: {lambda_rank_ic}")
        print(f"  Î»åˆ†å¸ƒæ­£åˆ™: {lambda_distribution}")
        print(f"  Î»æ–¹å·®ç¨³å®š: {lambda_variance}")
        
        # ä¿å­˜æŸå¤±æƒé‡ç”¨äºåŠ¨æ€è°ƒæ•´
        self.current_rank_ic_weight = lambda_rank_ic
    
    def compute_rank_ic_loss(self, factors: torch.Tensor, returns: torch.Tensor) -> torch.Tensor:
        """
        ç›´æ¥è®¡ç®—RankICæŸå¤±
        
        Args:
            factors: [num_stocks] å› å­å€¼
            returns: [num_stocks] æ”¶ç›Šç‡
        """
        # ç¡®ä¿æ•°æ®æœ‰æ•ˆ
        valid_mask = ~(torch.isnan(factors) | torch.isnan(returns))
        if valid_mask.sum() < 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆæ ·æœ¬
            return torch.tensor(0.0, device=factors.device, dtype=factors.dtype)
        
        valid_factors = factors[valid_mask]
        valid_returns = returns[valid_mask]
        
        # è®¡ç®—æ’åº
        factor_ranks = torch.argsort(torch.argsort(valid_factors)).float()
        return_ranks = torch.argsort(torch.argsort(valid_returns)).float()
        
        # è®¡ç®—Spearmanç›¸å…³ç³»æ•° (RankIC)
        n = len(factor_ranks)
        factor_ranks_centered = factor_ranks - factor_ranks.mean()
        return_ranks_centered = return_ranks - return_ranks.mean()
        
        numerator = torch.sum(factor_ranks_centered * return_ranks_centered)
        denominator = torch.sqrt(torch.sum(factor_ranks_centered**2) * torch.sum(return_ranks_centered**2))
        
        if denominator < self.eps:
            return torch.tensor(0.0, device=factors.device, dtype=factors.dtype)
        
        rank_ic = numerator / denominator
        
        # æŸå¤±ï¼šæˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–æ­£å‘RankIC
        # å¦‚æœRankICä¸ºè´Ÿï¼Œé¢å¤–æƒ©ç½šä»¥å¼ºåˆ¶æ­£å‘é¢„æµ‹
        if rank_ic < 0:
            rank_ic_loss = -rank_ic + 2.0 * torch.abs(rank_ic)  # é‡åº¦æƒ©ç½šè´ŸRankIC
        else:
            rank_ic_loss = -rank_ic  # æœ€å¤§åŒ–æ­£RankIC
        
        return rank_ic_loss
    
    def compute_current_rank_ic(self, factors: torch.Tensor, returns: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—å½“å‰çš„RankICå€¼ç”¨äºåŠ¨æ€æƒé‡è°ƒæ•´
        """
        # ç¡®ä¿æ•°æ®æœ‰æ•ˆ
        valid_mask = ~(torch.isnan(factors) | torch.isnan(returns))
        if valid_mask.sum() < 10:
            return torch.tensor(0.0, device=factors.device, dtype=factors.dtype)
        
        valid_factors = factors[valid_mask]
        valid_returns = returns[valid_mask]
        
        # è®¡ç®—æ’åº
        factor_ranks = torch.argsort(torch.argsort(valid_factors)).float()
        return_ranks = torch.argsort(torch.argsort(valid_returns)).float()
        
        # è®¡ç®—Spearmanç›¸å…³ç³»æ•°
        n = len(factor_ranks)
        factor_ranks_centered = factor_ranks - factor_ranks.mean()
        return_ranks_centered = return_ranks - return_ranks.mean()
        
        numerator = torch.sum(factor_ranks_centered * return_ranks_centered)
        denominator = torch.sqrt(torch.sum(factor_ranks_centered**2) * torch.sum(return_ranks_centered**2))
        
        if denominator < self.eps:
            return torch.tensor(0.0, device=factors.device, dtype=factors.dtype)
        
        rank_ic = numerator / denominator
        return rank_ic
    
    def compute_distribution_regularization(self, factors: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—åˆ†å¸ƒæ­£åˆ™åŒ–æŸå¤±ï¼Œé˜²æ­¢å› å­åˆ†å¸ƒåæ–œ
        """
        # æ ‡å‡†åŒ–å› å­
        factor_std = torch.std(factors) + self.eps
        factors_normalized = (factors - torch.mean(factors)) / factor_std
        
        # è®¡ç®—ååº¦æŸå¤± (å¸Œæœ›ååº¦æ¥è¿‘0)
        skewness = torch.mean((factors_normalized)**3)
        skewness_loss = torch.abs(skewness)
        
        # è®¡ç®—å³°åº¦æŸå¤± (å¸Œæœ›å³°åº¦æ¥è¿‘3ï¼Œå³æ­£æ€åˆ†å¸ƒ)
        kurtosis = torch.mean((factors_normalized)**4)
        kurtosis_loss = torch.abs(kurtosis - 3.0)
        
        # åˆ†å¸ƒæ­£åˆ™åŒ–æŸå¤±
        distribution_loss = skewness_loss + 0.5 * kurtosis_loss
        
        return distribution_loss
    
    def compute_variance_stability_loss(self, factors: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—æ–¹å·®ç¨³å®šæ€§æŸå¤±ï¼Œç¡®ä¿å› å­æ–¹å·®é€‚ä¸­
        """
        factor_var = torch.var(factors) + self.eps
        
        # æˆ‘ä»¬å¸Œæœ›æ–¹å·®åœ¨åˆç†èŒƒå›´å†… (0.5 - 2.0)
        target_var = 1.0
        var_loss = torch.abs(factor_var - target_var) / target_var
        
        return var_loss
    
    def compute_r_square_loss(self, factors: torch.Tensor, returns: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—æ”¹è¿›çš„R-squareæŸå¤±
        """
        # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
        if factors.dim() == 1:
            factors = factors.unsqueeze(-1)  # [num_stocks, 1]
        
        # æœ‰æ•ˆæ€§æ£€æŸ¥
        valid_mask = ~(torch.isnan(factors).any(dim=-1) | torch.isnan(returns))
        if valid_mask.sum() < 5:
            return torch.tensor(0.0, device=factors.device, dtype=factors.dtype)
        
        F_valid = factors[valid_mask]  # [valid_stocks, num_factors]
        y_valid = returns[valid_mask]  # [valid_stocks]
        
        try:
            # è®¡ç®— F^T F å’Œå…¶é€†çŸ©é˜µ
            FtF = torch.matmul(F_valid.t(), F_valid)  # [num_factors, num_factors]
            
            # æ·»åŠ æ­£åˆ™åŒ–é¡¹é˜²æ­¢å¥‡å¼‚
            ridge_reg = 1e-4 * torch.eye(FtF.shape[0], device=FtF.device)
            FtF_reg = FtF + ridge_reg
            
            # ä½¿ç”¨ä¼ªé€†æ›¿ä»£ç›´æ¥æ±‚é€†
            FtF_inv = torch.linalg.pinv(FtF_reg)
            
            # è®¡ç®—æŠ•å½±: P = F(F^T F)^{-1}F^T
            P = torch.matmul(torch.matmul(F_valid, FtF_inv), F_valid.t())
            
            # è®¡ç®—æŠ•å½±åçš„y: y_proj = P @ y
            y_proj = torch.matmul(P, y_valid)
            
            # è®¡ç®—æ®‹å·®
            residual = y_valid - y_proj
            
            # R-square = 1 - ||residual||Â² / ||y - mean(y)||Â²
            ss_res = torch.sum(residual**2)
            ss_tot = torch.sum((y_valid - torch.mean(y_valid))**2) + self.eps
            
            r_square = 1.0 - ss_res / ss_tot
            
            # æŸå¤±å‡½æ•°ï¼šå¸Œæœ›æœ€å¤§åŒ–R-square
            r_square_loss = -r_square
            
            return r_square_loss
            
        except Exception:
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè¿”å›å¤§çš„æŸå¤±å€¼
            return torch.tensor(10.0, device=factors.device, dtype=factors.dtype)
    
    def compute_orthogonal_penalty(self, factors: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—å› å­æ­£äº¤æ€§æƒ©ç½š
        """
        if factors.dim() == 1 or factors.shape[-1] == 1:
            return torch.tensor(0.0, device=factors.device, dtype=factors.dtype)
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        factors_centered = factors - torch.mean(factors, dim=0, keepdim=True)
        cov_matrix = torch.matmul(factors_centered.t(), factors_centered)
        
        # å¯¹è§’çº¿å½’ä¸€åŒ–
        std_vec = torch.sqrt(torch.diag(cov_matrix) + self.eps)
        corr_matrix = cov_matrix / torch.outer(std_vec, std_vec)
        
        # è®¡ç®—éå¯¹è§’çº¿å…ƒç´ çš„å¹³æ–¹å’Œ (å¸Œæœ›ä¸º0)
        mask = ~torch.eye(corr_matrix.shape[0], dtype=torch.bool, device=factors.device)
        
        if self.regularization_type == 'frobenius':
            penalty = torch.sum(corr_matrix[mask]**2)
        else:  # 'nuclear'
            penalty = torch.sum(torch.abs(corr_matrix[mask]))
        
        return penalty
    
    def forward(self, factors: torch.Tensor, future_returns_list: List[torch.Tensor]) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­è®¡ç®—æ€»æŸå¤±
        
        Args:
            factors: [num_stocks, num_factors] æˆ– [num_stocks] å› å­é¢„æµ‹
            future_returns_list: æœªæ¥å¤šæœŸæ”¶ç›Šç‡åˆ—è¡¨
        """
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´æ€§ - ä¿®å¤æ··åˆç²¾åº¦è®­ç»ƒé—®é¢˜
        total_loss = torch.tensor(0.0, device=factors.device, dtype=factors.dtype, requires_grad=True)
        
        # ç¡®ä¿factorsæ˜¯2D
        if factors.dim() == 1:
            factors = factors.unsqueeze(-1)
        
        num_periods = min(len(future_returns_list), self.max_periods)
        
        for t in range(num_periods):
            returns_t = future_returns_list[t]
            time_weight = self.omega ** t
            
            # å¯¹å•å› å­æƒ…å†µçš„ç‰¹æ®Šå¤„ç†
            if factors.shape[-1] == 1:
                factor_t = factors[:, 0]  # [num_stocks]
            else:
                factor_t = factors[:, 0]  # å–ç¬¬ä¸€ä¸ªå› å­
            
            # 1. RankICæŸå¤± (æœ€é‡è¦)
            rank_ic_loss = self.compute_rank_ic_loss(factor_t, returns_t)
            
            # åŠ¨æ€è°ƒæ•´RankICæƒé‡ - å¦‚æœå½“å‰é¢„æµ‹æ•ˆæœä¸ä½³ï¼ŒåŠ å¤§æƒ©ç½š
            current_rank_ic = self.compute_current_rank_ic(factor_t, returns_t)
            dynamic_rank_ic_weight = self.lambda_rank_ic
            if current_rank_ic < 0:
                dynamic_rank_ic_weight *= 2.5  # RankICä¸ºè´Ÿæ—¶æƒé‡åŠ å¼º
            
            # 2. R-squareæŸå¤±
            r_square_loss = self.compute_r_square_loss(factors, returns_t)
            
            # 3. åˆ†å¸ƒæ­£åˆ™åŒ–æŸå¤±
            distribution_loss = self.compute_distribution_regularization(factor_t)
            
            # 4. æ–¹å·®ç¨³å®šæ€§æŸå¤±
            variance_loss = self.compute_variance_stability_loss(factor_t)
            
            # ç»„åˆæŸå¤± - åŠ å¼ºRankICæƒé‡
            period_loss = (
                dynamic_rank_ic_weight * rank_ic_loss +
                r_square_loss +
                self.lambda_distribution * distribution_loss +
                self.lambda_variance * variance_loss
            )
            
            total_loss = total_loss + time_weight * period_loss
        
        # 5. æ­£äº¤æ€§æƒ©ç½š (å¦‚æœæ˜¯å¤šå› å­)
        if factors.shape[-1] > 1:
            orthogonal_penalty = self.compute_orthogonal_penalty(factors)
            total_loss = total_loss + self.lambda_orthogonal * orthogonal_penalty
        
        return total_loss 